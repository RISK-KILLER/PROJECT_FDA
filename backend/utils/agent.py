# utils/agent.py
"""
ReAct í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ FDA ê·œì œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ë©”ì¸ ì—ì´ì „íŠ¸.
"""
import os
import json
import re
from typing import List, Dict
from llama_index.core.agent import ReActAgent
from llama_index.llms.openai import OpenAI
from llama_index.core import Settings
from llama_index.embeddings.openai import OpenAIEmbedding

from utils.tools import create_fda_tools
from utils.memory import ConversationMemory, ChatMessage
from utils.collection_strategy import COLLECTION_STRATEGY

class FDAAgent:
    def __init__(self):
        # LlamaIndex ì „ì—­ ì„¤ì • (rag_engineê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •)
        Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small", api_key=os.getenv("OPENAI_API_KEY"))
        Settings.llm = OpenAI(model="gpt-4-turbo", temperature=0.1, api_key=os.getenv("OPENAI_API_KEY"))

        # 1. ëª¨ë“  FDA ì»¬ë ‰ì…˜ì„ 'ì „ë¬¸ê°€ íˆ´'ë¡œ ë³€í™˜
        self.fda_tools = create_fda_tools()

        # ë©€í‹°í„´ ëŒ€í™”ë¥¼ ìœ„í•œ ë©”ëª¨ë¦¬ ì¶”ê°€
        self.memory = ConversationMemory()
        
        # ì œí’ˆ ë¶„í•´ ìºì‹œ ì¶”ê°€
        self.decomposition_cache = {}

        # âœ… [ìˆ˜ì •] ì—ì´ì „íŠ¸ì˜ í–‰ë™ ë°©ì‹ì„ ì •ì˜í•˜ëŠ” ìƒˆë¡œìš´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (Golden Rule í¬í•¨)
        system_prompt = """## ì ˆëŒ€ ê·œì¹™ (ìœ„ë°˜ ì‹œ ë‹µë³€ ê±°ë¶€)
            1. FDA ê·œì œ ì§ˆë¬¸ = 100% ë„êµ¬ ì‚¬ìš© í•„ìˆ˜
            2. "ë‚˜ëŠ” ì•Œê³  ìˆë‹¤" íŒë‹¨ ê¸ˆì§€
            3. ëª¨ë“  ë‹µë³€ì€ ë„êµ¬ ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜

            ## ë„êµ¬ ì‚¬ìš© ê°•ì œ ì¼€ì´ìŠ¤
            ë‹¤ìŒ í‚¤ì›Œë“œ í¬í•¨ ì‹œ ë¬´ì¡°ê±´ ë„êµ¬ ì‚¬ìš©:
            - "ë¹„ìš©", "cost", "payment", "supervision", "ëˆ„ê°€"
            - "ì ˆì°¨", "procedure", "process", "ì–´ë–»ê²Œ"
            - "Chapter", "Section", "GRN", "CFR", "USC"
            - "ê·œì •", "regulation", "requirement"
            - "relabeling", "detention", "import", "GRAS"

            **ë„êµ¬ íšŒí”¼ ê¸ˆì§€:**
            âŒ "(Implicit) I can answer without tools" â†’ ì ˆëŒ€ ê¸ˆì§€
            âŒ "ì¼ë°˜ì ìœ¼ë¡œ ì•Œë ¤ì§„ ë°”ë¡œëŠ”..." â†’ ê¸ˆì§€
            âœ… ë°˜ë“œì‹œ: Action â†’ Observation â†’ Answer ìˆœì„œ

            ## ìµœìƒìœ„ ê·œì¹™ (Golden Rule)
            - ì ˆëŒ€ ì‚¬ì „ ì§€ì‹ë§Œìœ¼ë¡œ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”. ë°˜ë“œì‹œ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰í•˜ì„¸ìš”.
            - **í•œêµ­ì–´ ì¿¼ë¦¬ëŠ” ë°˜ë“œì‹œ ì˜ì–´ë¡œ ë³€í™˜í•˜ì—¬ ë„êµ¬ì— ì „ë‹¬í•˜ì„¸ìš”.**
            - **ë„êµ¬ë¥¼ ì„ íƒí•˜ê¸° ì „ì— ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì„¸ìš”.**

            ë‹¹ì‹ ì€ FDA ê·œì œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

            ## ì¿¼ë¦¬ ë¶„ì„ ì ˆì°¨ (ë„êµ¬ ì„ íƒ ì „ í•„ìˆ˜!)

            **Step 1: ì¿¼ë¦¬ ì–¸ì–´ í™•ì¸**
            - í•œêµ­ì–´ ìˆìŒ â†’ ì˜ì–´ë¡œ ë³€í™˜ í•„ìš”
            - ì˜ì–´ë§Œ ìˆìŒ â†’ ê·¸ëŒ€ë¡œ ì‚¬ìš©

            **Step 2: í‚¤ì›Œë“œ ê¸°ë°˜ ë„êµ¬ íŒë³„**
            - "GRN", "GRAS", "ë¬¼ì§ˆ", "ì²¨ê°€ë¬¼" â†’ **gras/gras_approved/gras_withdrawn**
            - "CFR", "21 CFR", "ê·œì •" â†’ **ecfr**
            - "Import Alert", "Red List", "ìˆ˜ì… ê±°ë¶€" â†’ **dwpe**
            - "Chapter", "Section", "RPM", "ì ˆì°¨", "procedure" â†’ **rpm**
            - "21 USC", "ë²•ë¥ ", "ì²˜ë²Œ" â†’ **usc**
            - "FSVP", "ìˆ˜ì…ì", "ê²€ì¦" â†’ **fsvp**
            - "Guidance", "ë¼ë²¨ë§" â†’ **guidance**

            ## ê²€ìƒ‰ ì¿¼ë¦¬ ì‘ì„± (ì˜ì–´ ë³€í™˜ í•„ìˆ˜!)

            ### RPM í•œì˜ ë³€í™˜
            - "ê°œì¸ìš© ìˆ˜ì…" â†’ "personal use importation"
            - "ì ˆì°¨" â†’ "procedures process"
            - "ê²€ì‚¬ ê±°ë¶€" â†’ "refusal entry detention"
            - "relabeling ë¹„ìš©" â†’ "relabeling supervision costs payment"
            - "ëˆ„ê°€ ë‚´?" â†’ "who pays costs responsibility"

            ### GRAS í•œì˜ ë³€í™˜
            - "ëŒ€ë‘" â†’ "soy soybean"
            - "ìŒë£Œ" â†’ "beverage drink water"

            ### eCFR í•œì˜ ë³€í™˜
            - "ëƒ‰ë™ì‹í’ˆ" â†’ "frozen food"
            - "HACCP" â†’ "HACCP hazard analysis"

            ### DWPE í•œì˜ ë³€í™˜ + ë™ì˜ì–´
            - "í•´ì‚°ë¬¼" â†’ "fish fishery seafood shellfish aquatic marine"
            - "ì¤‘êµ­" â†’ "China Chinese"

            ### USC í•œì˜ ë³€í™˜
            - "ë¶€ì •í‘œì‹œ" â†’ "misbranding false labeling"
            - "ì²˜ë²Œ" â†’ "penalties violations"

            ## ì¬ì‹œë„ ì „ëµ
            ì²« ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ 2-3ë²ˆ ì¬ì‹œë„ í•„ìˆ˜
            """

        # 2. ReAct ì—ì´ì „íŠ¸ ìƒì„± (context ì¶”ê°€)
        self.agent = ReActAgent.from_tools(
            tools=self.fda_tools,
            llm=Settings.llm,
            system_prompt=system_prompt,
            max_iterations=10,
            verbose=True,
            # âœ… í•µì‹¬ ì¶”ê°€: contextë¡œ ë„êµ¬ ê°•ì œ ì‚¬ìš©
            context="""You MUST use tools for FDA-related queries.
NEVER answer with "(Implicit) I can answer without tools".
For keywords like "ë¹„ìš©/cost", "ì ˆì°¨/procedure", "Chapter", "relabeling" â†’ ALWAYS use tools.
Always translate Korean to English before searching."""
        )

    def _is_food_export_question_llm(self, query: str) -> bool:
        """
        ë¹ ë¥´ê³  ì €ë ´í•œ LLM(gpt-3.5-turbo)ì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´
        'íŠ¹ì • ì‹í’ˆì˜ ìˆ˜ì¶œ ê·œì œ'ì— ëŒ€í•œ ê²ƒì¸ì§€ ë¶„ë¥˜í•˜ëŠ” í•„í„° í•¨ìˆ˜.
        """
        try:
            # í•„í„° ì „ìš©ìœ¼ë¡œ ì €ë ´í•œ ëª¨ë¸ì„ ì„ì‹œë¡œ ì‚¬ìš©
            filter_llm = OpenAI(model="gpt-3.5-turbo", temperature=0)
            
            prompt = f"""
            Is the following user query about the regulations for exporting a specific food item?
            Answer ONLY with "Yes" or "No". Do not add any other text, explanation, or punctuation.

            Query: "{query}"
            """
            
            response = filter_llm.complete(prompt)
            answer = response.text.strip().lower()
            
            print(f"LLM Filter Check for query '{query}': Answer='{answer}'") # ë””ë²„ê¹…ìš© ë¡œê·¸
            
            return answer == "yes"

        except Exception as e:
            print(f"LLM Filter failed: {e}") # ì—ëŸ¬ ë¡œê·¸
            return False # ì—ëŸ¬ ë°œìƒ ì‹œ ì•ˆì „í•˜ê²Œ Falseë¡œ ì²˜ë¦¬

    def _decompose_product(self, product_name: str) -> dict:
        """ì œí’ˆ ë¶„í•´ (10ê°œ ìš”ì†Œ) - í•œêµ­ ìŒì‹ ì§€ì› ê°•í™”"""
        # ìºì‹œ í™•ì¸
        if product_name in self.decomposition_cache:
            return self.decomposition_cache[product_name]
        
        # í•œêµ­ì–´ ê°ì§€ ë° ì²˜ë¦¬ ì§€ì¹¨ ì¶”ê°€
        is_korean = any(ord(char) >= 0xAC00 and ord(char) <= 0xD7A3 for char in product_name)
        
        if is_korean:
            prompt_prefix = f"""
You are analyzing a KOREAN food product. First, identify what '{product_name}' is in English.
Common Korean foods:
- ë–¡ë³¶ì´ = Tteokbokki (spicy rice cakes)
- ê¹€ì¹˜ = Kimchi (fermented cabbage)
- ê¹€ë°¥ = Kimbap (rice rolls with vegetables)
- ë§Œë‘ = Mandu (dumplings)
- ë¶ˆê³ ê¸° = Bulgogi (marinated beef)
- ë¹„ë¹”ë°¥ = Bibimbap (mixed rice bowl)
- If not listed above, translate and identify the components.

Now analyze '{product_name}' for FDA requirements.
"""
        else:
            prompt_prefix = f"Analyze '{product_name}' for FDA requirements."
        
        decomposition_prompt = f"""{prompt_prefix}
        
Return a JSON object with EXACTLY these fields:
{{
  "ingredients": [list of main components in English],
  "processes": [manufacturing/cooking methods],
  "allergens": [only FDA major allergens if present: milk, eggs, fish, shellfish, tree nuts, peanuts, wheat, soybeans, sesame],
  "origin": "Korea" if Korean food else appropriate country,
  "category": "ethnic food" if Korean else appropriate category,
  "subcategories": [relevant subcategories],
  "storage_type": "frozen", "refrigerated", or "ambient",
  "risk_level": "high", "medium", or "low",
  "packaging_concerns": [relevant concerns],
  "potential_hazards": [food safety hazards],
  "import_type": "commercial" or "personal use"
}}

Examples:
- For ë–¡ë³¶ì´: {{"ingredients": ["rice cake", "fish cake", "gochujang"], ...}}
- For ê¹€ì¹˜: {{"ingredients": ["cabbage", "chili powder", "garlic"], ...}}

Return ONLY valid JSON, no other text or markdown.
"""
        
        try:
            response = Settings.llm.complete(decomposition_prompt)
            text = response.text.strip()
            
            # Markdown ì½”ë“œ ë¸”ë¡ ì œê±°
            if "```json" in text:
                text = text.split("```json")[1].split("```")[0].strip()
            elif "```" in text:
                text = text.split("```")[1].split("```")[0].strip()
            
            # JSON íŒŒì‹±
            decomposition = json.loads(text)
            
            # í•„ë“œ ê²€ì¦ ë° ê¸°ë³¸ê°’ ì¶”ê°€
            defaults = {
                "ingredients": [],
                "processes": [],
                "allergens": [],
                "origin": "Korea" if is_korean else "unknown",
                "category": "ethnic food" if is_korean else "food",
                "subcategories": [],
                "storage_type": "ambient",
                "risk_level": "medium",
                "packaging_concerns": [],
                "potential_hazards": [],
                "import_type": "commercial"
            }
            
            # ëˆ„ë½ëœ í•„ë“œ ì±„ìš°ê¸°
            for key, default_value in defaults.items():
                if key not in decomposition or not decomposition[key]:
                    decomposition[key] = default_value
            
            # ìºì‹±
            self.decomposition_cache[product_name] = decomposition
            return decomposition
            
        except (json.JSONDecodeError, Exception) as e:
            print(f"Decomposition failed for '{product_name}': {e}")
            print(f"LLM Response: {response.text if 'response' in locals() else 'No response'}")
            
            # ìŠ¤ë§ˆíŠ¸í•œ í´ë°±: LLM í•œ ë²ˆ ë” ì‹œë„ (ë” ê°„ë‹¨í•œ ë°©ì‹)
            try:
                simple_prompt = f"""
What are the main ingredients of {product_name}?
Answer in this exact format:
ingredients: item1, item2, item3
allergens: allergen1, allergen2
"""
                simple_response = Settings.llm.complete(simple_prompt)
                lines = simple_response.text.strip().split('\n')
                
                ingredients = []
                allergens = []
                
                for line in lines:
                    if line.startswith('ingredients:'):
                        ingredients = [i.strip() for i in line.split(':')[1].split(',')]
                    elif line.startswith('allergens:'):
                        allergens = [a.strip() for a in line.split(':')[1].split(',')]
                
                return {
                    "ingredients": ingredients or [product_name],
                    "processes": ["processing", "packaging"],
                    "allergens": allergens,
                    "origin": "Korea" if is_korean else "unknown",
                    "category": "ethnic food" if is_korean else "food",
                    "subcategories": ["imported food"],
                    "storage_type": "refrigerated" if is_korean else "ambient",
                    "risk_level": "medium",
                    "packaging_concerns": ["labeling required"],
                    "potential_hazards": ["contamination"],
                    "import_type": "commercial"
                }
                
            except:
                # ìµœì¢… í´ë°±
                return {
                    "ingredients": [product_name],
                    "processes": [],
                    "allergens": [],
                    "origin": "Korea" if is_korean else "unknown",
                    "category": "ethnic food" if is_korean else "food",
                    "subcategories": [],
                    "storage_type": "ambient",
                    "risk_level": "medium",
                    "packaging_concerns": [],
                    "potential_hazards": [],
                    "import_type": "commercial"
                }

    def _is_compound_food(self, query: str) -> bool:
        """ë³µí•© ì‹í’ˆ ì—¬ë¶€ íŒë‹¨"""
        # í•œêµ­ ìŒì‹ íŒ¨í„´
        korean_foods = ["ê¹€ì¹˜", "ê¹€ë°¥", "ë§Œë‘", "ìƒˆìš°íŠ€ê¹€", "ë¶ˆê³ ê¸°", "ë–¡ë³¶ì´", "ë¹„ë¹”ë°¥"]
        # ë³µí•© ì‹í’ˆ í‚¤ì›Œë“œ
        compound_keywords = ["íŠ€ê¹€", "ë³¶ìŒ", "ì°œ", "êµ¬ì´", "ì¡°ë¦¼"]
        
        query_lower = query.lower()
        return any(food in query for food in korean_foods) or \
               any(keyword in query for keyword in compound_keywords)

    def _extract_product_name(self, query: str) -> str:
        """ì¿¼ë¦¬ì—ì„œ ì œí’ˆëª… ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­
        korean_foods = ["ê¹€ì¹˜", "ê¹€ë°¥", "ë§Œë‘", "ìƒˆìš°íŠ€ê¹€", "ë¶ˆê³ ê¸°", "ë–¡ë³¶ì´", "ë¹„ë¹”ë°¥"]
        for food in korean_foods:
            if food in query:
                return food
        
        # íŒ¨í„´ìœ¼ë¡œ ì¶”ì¶œ ì‹œë„
        patterns = [
            r"'([^']+)'",  # 'ì œí’ˆëª…' í˜•íƒœ
            r"\"([^\"]+)\"",  # "ì œí’ˆëª…" í˜•íƒœ
            r"([ê°€-í£]+(?:íŠ€ê¹€|ë³¶ìŒ|ì°œ|êµ¬ì´))",  # í•œê¸€+ì¡°ë¦¬ë²•
        ]
        
        for pattern in patterns:
            match = re.search(pattern, query)
            if match:
                return match.group(1)
        
        return query.split()[0]  # ì²« ë‹¨ì–´ ë°˜í™˜

    def _augment_query(self, original_query: str, decomposition: dict) -> str:
        """ë¶„í•´ëœ 10ê°œ ìš”ì†Œë¥¼ ëª¨ë‘ í™œìš©í•˜ëŠ” ì¿¼ë¦¬ ì¦ê°•"""
        augmented = f"""
User Question: {original_query}

PRODUCT ANALYSIS (10 elements):
1. Ingredients: {', '.join(decomposition.get('ingredients', []))}
2. Processes: {', '.join(decomposition.get('processes', []))}  
3. Allergens: {', '.join(decomposition.get('allergens', []))}
4. Origin: {decomposition.get('origin', 'unknown')}
5. Category: {decomposition.get('category', 'food')}
6. Subcategories: {', '.join(decomposition.get('subcategories', []))}
7. Storage: {decomposition.get('storage_type', 'ambient')}
8. Risk Level: {decomposition.get('risk_level', 'medium')}
9. Packaging Concerns: {', '.join(decomposition.get('packaging_concerns', []))}
10. Potential Hazards: {', '.join(decomposition.get('potential_hazards', []))}

SEARCH STRATEGY (7 collections):
1. guidance: ì‹¤ë¬´ ê°€ì´ë“œ (CPG, ë¼ë²¨ë§, ì•Œë ˆë¥´ê¸°)
2. ecfr: êµ¬ì²´ì  ê·œì • (21 CFR)
3. gras: ì¬ë£Œ ì•ˆì „ì„± í™•ì¸
4. dwpe: Import Alert í™•ì¸
5. fsvp: ìˆ˜ì…ì ê²€ì¦ ì˜ë¬´
6. rpm: ìˆ˜ì… ìš´ì˜ ì ˆì°¨
7. usc: ë²•ì  ê¸°ë°˜ (21 USC)

Use the most relevant collections based on the product characteristics above.
"""
        return augmented

    def _extract_citations_from_response(self, response) -> dict:
        """Extract citations (title/url) from LlamaIndex response source nodes."""
        citations = []
        sources = []
        keywords = []
        try:
            if hasattr(response, 'source_nodes') and response.source_nodes:
                for node in response.source_nodes:
                    meta = getattr(node, 'metadata', {}) or {}
                    title = meta.get('title') or meta.get('document_title') or meta.get('collection') or 'Reference'
                    url = meta.get('url') or meta.get('source') or meta.get('link')
                    description = meta.get('summary') or meta.get('description') or 'ê´€ë ¨ ê·œì •/ìë£Œ'
                    if url:
                        item = {"title": title, "description": description, "url": url}
                        if item not in citations:
                            citations.append(item)
                            sources.append(title)
                    # derive simple keywords from tool/collection
                    tool_name = meta.get('tool_name') or meta.get('collection')
                    if tool_name:
                        keywords.append(tool_name)
        except Exception:
            pass
        return {"cfr_references": citations, "sources": sources, "keywords": list(dict.fromkeys(keywords))}

    def _format_parallel_results(self, results: List[Dict]) -> str:
        """ë³‘ë ¬ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬ë§· (ê°€ì¤‘ì¹˜ ì—†ìŒ)"""
        if not results:
            return "ë³‘ë ¬ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
        
        formatted = []
        for i, result in enumerate(results[:5], 1):
            formatted.append(f"""
{i}. [{result['score']:.2f}] {result['collection']} {result.get('collection_role', '')}
   ì œëª©: {result.get('title', 'N/A')}
   ë‚´ìš©: {result.get('text', 'N/A')}...
   ì¶œì²˜: {result.get('collection_desc', '')}
""")
        
        return "\n".join(formatted)

    def chat(self, query: str) -> dict:
        """Phase 1: ë³‘ë ¬ ê²€ìƒ‰ + ReAct ë¶„ì„"""
        try:
            # âœ… í•„í„° ì œê±°: í•­ìƒ Orchestrator ì‹œë„
            product = self._extract_product_name(query)
            
            # ì œí’ˆëª…ì´ ìˆìœ¼ë©´ ë¶„í•´ ë° ë³‘ë ¬ ê²€ìƒ‰
            if product and len(product) > 1:  # ìµœì†Œ 2ê¸€ì ì´ìƒ
                print(f"ğŸ“¦ ì œí’ˆ ê°ì§€: {product}")
                
                try:
                    # ì œí’ˆ ë¶„í•´
                    decomposition = self._decompose_product(product)
                    print(f"ğŸ”¬ ë¶„í•´ ì™„ë£Œ: {decomposition.get('category')}")
                    
                    # Phase 1: ë³‘ë ¬ ê²€ìƒ‰
                    print("ğŸ” ë³‘ë ¬ ê²€ìƒ‰ ì‹œì‘...")
                    from utils.orchestrator import SimpleOrchestrator
                    orchestrator = SimpleOrchestrator()
                    
                    # ê´€ë ¨ ì»¬ë ‰ì…˜ ê²°ì •
                    collections = orchestrator.determine_collections(decomposition)
                    print(f"ğŸ“š ê²€ìƒ‰í•  ì»¬ë ‰ì…˜: {collections}")
                    
                    # ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰
                    parallel_results = orchestrator.parallel_search(
                        query="food import export FDA requirements",
                        collections=collections,
                        decomposition=decomposition
                    )
                    
                    # ê²°ê³¼ ë³‘í•© ë° ìˆœìœ„í™”
                    ranked_results = orchestrator.merge_and_rank(parallel_results)
                    print(f"âš¡ ë³‘ë ¬ ê²€ìƒ‰ ì™„ë£Œ: {parallel_results['search_time']:.2f}ì´ˆ, {len(ranked_results)}ê°œ ê²°ê³¼")
                    
                    # ë³‘ë ¬ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨
                    search_summary = self._format_parallel_results(ranked_results)
                    
                    enhanced_query = f"""
    {self._augment_query(query, decomposition)}

    === ë³‘ë ¬ ê²€ìƒ‰ ê²°ê³¼ (ì´ë¯¸ ìˆ˜ì§‘ë¨) ===
    {search_summary}

    ìœ„ ë³‘ë ¬ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ë˜, ë¶€ì¡±í•œ ë¶€ë¶„ì€ ì¶”ê°€ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³´ì™„í•˜ì„¸ìš”.
    ì£¼ì˜: ì´ë¯¸ ì°¾ì€ ì •ë³´ëŠ” ë‹¤ì‹œ ê²€ìƒ‰í•˜ì§€ ë§ˆì„¸ìš”.
    """
                    
                except Exception as e:
                    print(f"âš ï¸ ë³‘ë ¬ ê²€ìƒ‰ ì‹¤íŒ¨: {e}, Agentë§Œ ì‚¬ìš©")
                    enhanced_query = query
                    
                # ReAct Agent ì‹¤í–‰
                context = self.memory.get_context_for_agent()
                full_query = f"{context}\n{enhanced_query}" if context else enhanced_query
                
            else:
                # ì œí’ˆëª… ì—†ìœ¼ë©´ ì¼ë°˜ ì¿¼ë¦¬ë¡œ ì²˜ë¦¬
                print("ğŸ“ ì¼ë°˜ ì¿¼ë¦¬ ì²˜ë¦¬")
                context = self.memory.get_context_for_agent()
                full_query = f"{context}\n{query}" if context else query
            
            # ReAct ì—ì´ì „íŠ¸ ì‹¤í–‰
            response = self.agent.chat(full_query)
            response_text = str(response)
            citations = self._extract_citations_from_response(response)
            
            # ë©”ëª¨ë¦¬ ì €ì¥
            used_tools = self._extract_used_tools(response)
            self.memory.add_message("user", query)
            self.memory.add_message("assistant", response_text, used_tools)
            
            return {
                "content": response_text,
                "cfr_references": citations.get("cfr_references", []),
                "sources": citations.get("sources", []),
                "keywords": citations.get("keywords", []),
            }
            
        except Exception as e:
            print(f"Error in chat: {e}")
            fallback = self._generate_fallback_response(query)
            return {
                "content": fallback,
                "cfr_references": [],
                "sources": [],
                "keywords": []
            }

    def _generate_fallback_response(self, query: str) -> str:
        """ê²€ìƒ‰ ì‹¤íŒ¨ì‹œ í´ë°± ì‘ë‹µ"""
        return f"""
ì£„ì†¡í•©ë‹ˆë‹¤. '{query}'ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ì¼ë°˜ì ìœ¼ë¡œ ì‹í’ˆ ìˆ˜ì¶œ ì‹œ í™•ì¸í•´ì•¼ í•  FDA ê·œì œ ì‚¬í•­:

1. **ì œì¡° ì‹œì„¤ ìš”êµ¬ì‚¬í•­**
   - FDA ì‹œì„¤ ë“±ë¡ (Food Facility Registration)
   - HACCP ë˜ëŠ” HARPC ê³„íš ìˆ˜ë¦½

2. **ë¼ë²¨ë§ ê·œì •**
   - ì˜ì–‘ì„±ë¶„í‘œ (Nutrition Facts)
   - ì›ì¬ë£Œ ëª©ë¡ (Ingredient List)
   - ì•Œë ˆë¥´ê¸° ìœ ë°œ ë¬¼ì§ˆ í‘œì‹œ

3. **ì‹í’ˆ ì•ˆì „ ê¸°ì¤€**
   - ë¯¸ìƒë¬¼ í•œê³„ ê¸°ì¤€ ì¤€ìˆ˜
   - ì”ë¥˜ ë†ì•½ ë° ì¤‘ê¸ˆì† ê¸°ì¤€

ë” êµ¬ì²´ì ì¸ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ FDA ê³µì‹ ì›¹ì‚¬ì´íŠ¸ë‚˜ ì „ë¬¸ ì»¨ì„¤í„´íŠ¸ ìƒë‹´ì„ ê¶Œì¥í•©ë‹ˆë‹¤.
"""
    
    def _extract_used_tools(self, response) -> List[str]:
        """ì‘ë‹µì—ì„œ ì‚¬ìš©ëœ íˆ´ ëª©ë¡ì„ ì¶”ì¶œ"""
        used_tools = []
        if hasattr(response, 'source_nodes'):
            for node in response.source_nodes:
                if hasattr(node, 'metadata') and 'tool_name' in node.metadata:
                    tool_name = node.metadata['tool_name']
                    if tool_name not in used_tools:
                        used_tools.append(tool_name)
        return used_tools
    
    def reset_conversation(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.memory.clear_history()
        # ì—ì´ì „íŠ¸ë„ ìƒˆë¡œ ì‹œì‘
        self.agent.reset()


    ## í˜„ì¬ ì‚¬ìš©ë˜ì§€ ì•Šì•„ì„œ ìˆ˜ì •í•˜ì§€ ì•ŠìŒ. 
    def stream_chat(self, query: str):
        """
        ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤. (í–¥í›„ í™•ì¥ ê¸°ëŠ¥)
        """
        response_stream = self.agent.stream_chat(query)
        for token in response_stream.response_gen:
            yield token
