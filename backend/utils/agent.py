# utils/agent.py
"""
ReAct í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ FDA ê·œì œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ë©”ì¸ ì—ì´ì „íŠ¸.
"""
import os
import json
import re
from typing import List, Dict
from llama_index.core.agent import ReActAgent
from llama_index.llms.openai import OpenAI
from llama_index.core import Settings
from llama_index.embeddings.openai import OpenAIEmbedding

from utils.tools import create_fda_tools
from utils.memory import ConversationMemory, ChatMessage
from utils.collection_strategy import COLLECTION_STRATEGY

class FDAAgent:
    def __init__(self):
        # LlamaIndex ì „ì—­ ì„¤ì • (rag_engineê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •)
        Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small", api_key=os.getenv("OPENAI_API_KEY"))
        Settings.llm = OpenAI(model="gpt-4-turbo", temperature=0.1, api_key=os.getenv("OPENAI_API_KEY"))

        # 1. ëª¨ë“  FDA ì»¬ë ‰ì…˜ì„ 'ì „ë¬¸ê°€ íˆ´'ë¡œ ë³€í™˜
        self.fda_tools = create_fda_tools()

        # ë©€í‹°í„´ ëŒ€í™”ë¥¼ ìœ„í•œ ë©”ëª¨ë¦¬ ì¶”ê°€
        self.memory = ConversationMemory()
        
        # ì œí’ˆ ë¶„í•´ ìºì‹œ ì¶”ê°€
        self.decomposition_cache = {}

        # âœ… [ìˆ˜ì •] ì—ì´ì „íŠ¸ì˜ í–‰ë™ ë°©ì‹ì„ ì •ì˜í•˜ëŠ” ìƒˆë¡œìš´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì •ë³´ ìˆ˜ì§‘ ì „ìš©)
        system_prompt = """ë‹¹ì‹ ì€ FDA ê·œì œ ì •ë³´ ìˆ˜ì§‘ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## ì—­í• 
ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ í•„ìš”í•œ ì •ë³´ë¥¼ ë„êµ¬ë¡œ ìˆ˜ì§‘í•˜ì„¸ìš”.
ìµœì¢… ë‹µë³€ì€ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”. ì •ë³´ë§Œ ìˆ˜ì§‘í•˜ì„¸ìš”.

## ìˆ˜ì§‘í•´ì•¼ í•  ì •ë³´
1. CFR ê·œì • (êµ¬ì²´ì  ë²ˆí˜¸ + ë‚´ìš©)
2. Import Alert í™•ì¸
3. ë¼ë²¨ë§ ìš”êµ¬ì‚¬í•­
4. FSVP/ê²€ì¦ ì ˆì°¨
5. ê¸°íƒ€ ê´€ë ¨ ê·œì œ ì •ë³´

## ì¶œë ¥ í˜•ì‹
ìˆ˜ì§‘í•œ ì •ë³´ë¥¼ êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”:

**CFR ê·œì •:**
- [ê·œì • ë²ˆí˜¸]: [ë‚´ìš©]

**Import Alert:**
- [Alert ë²ˆí˜¸]: [ë‚´ìš©]

**ë¼ë²¨ë§:**
- [ìš”êµ¬ì‚¬í•­]

**FSVP:**
- [ì ˆì°¨]

## ì¤‘ìš”
- ì œê³µëœ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš°ì„  ì°¸ê³ í•˜ì„¸ìš”
- ë¶€ì¡±í•œ ì •ë³´ë§Œ ë„êµ¬ë¡œ ì¶”ê°€ ê²€ìƒ‰í•˜ì„¸ìš”
- í•œêµ­ì–´ ì¿¼ë¦¬ëŠ” ë°˜ë“œì‹œ ì˜ì–´ë¡œ ë³€í™˜í•˜ì„¸ìš”

## ë„êµ¬ ì‚¬ìš© ê°•ì œ ì¼€ì´ìŠ¤
ë‹¤ìŒ í‚¤ì›Œë“œ í¬í•¨ ì‹œ ë¬´ì¡°ê±´ ë„êµ¬ ì‚¬ìš©:
- "ë¹„ìš©", "cost", "payment", "supervision", "ëˆ„ê°€"
- "ì ˆì°¨", "procedure", "process", "ì–´ë–»ê²Œ"
- "Chapter", "Section", "GRN", "CFR", "USC"
- "ê·œì •", "regulation", "requirement"
- "relabeling", "detention", "import", "GRAS"

**ë„êµ¬ íšŒí”¼ ê¸ˆì§€:**
âŒ "(Implicit) I can answer without tools" â†’ ì ˆëŒ€ ê¸ˆì§€
âŒ "ì¼ë°˜ì ìœ¼ë¡œ ì•Œë ¤ì§„ ë°”ë¡œëŠ”..." â†’ ê¸ˆì§€
âœ… ë°˜ë“œì‹œ: Action â†’ Observation â†’ Answer ìˆœì„œ

## ìµœìƒìœ„ ê·œì¹™ (Golden Rule)
- ì ˆëŒ€ ì‚¬ì „ ì§€ì‹ë§Œìœ¼ë¡œ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”. ë°˜ë“œì‹œ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰í•˜ì„¸ìš”.
- **í•œêµ­ì–´ ì¿¼ë¦¬ëŠ” ë°˜ë“œì‹œ ì˜ì–´ë¡œ ë³€í™˜í•˜ì—¬ ë„êµ¬ì— ì „ë‹¬í•˜ì„¸ìš”.**
- **ë„êµ¬ë¥¼ ì„ íƒí•˜ê¸° ì „ì— ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì„¸ìš”.**

## ì¿¼ë¦¬ ë¶„ì„ ì ˆì°¨ (ë„êµ¬ ì„ íƒ ì „ í•„ìˆ˜!)

**Step 1: ì¿¼ë¦¬ ì–¸ì–´ í™•ì¸**
- í•œêµ­ì–´ ìˆìŒ â†’ ì˜ì–´ë¡œ ë³€í™˜ í•„ìš”
- ì˜ì–´ë§Œ ìˆìŒ â†’ ê·¸ëŒ€ë¡œ ì‚¬ìš©

**Step 2: í‚¤ì›Œë“œ ê¸°ë°˜ ë„êµ¬ íŒë³„**
- "GRN", "GRAS", "ë¬¼ì§ˆ", "ì²¨ê°€ë¬¼" â†’ **gras/gras_approved/gras_withdrawn**
- "CFR", "21 CFR", "ê·œì •" â†’ **ecfr**
- "Import Alert", "Red List", "ìˆ˜ì… ê±°ë¶€" â†’ **dwpe**
- "Chapter", "Section", "RPM", "ì ˆì°¨", "procedure" â†’ **rpm**
- "21 USC", "ë²•ë¥ ", "ì²˜ë²Œ" â†’ **usc**
- "FSVP", "ìˆ˜ì…ì", "ê²€ì¦" â†’ **fsvp**
- "Guidance", "ë¼ë²¨ë§" â†’ **guidance**

## ê²€ìƒ‰ ì¿¼ë¦¬ ì‘ì„± (ì˜ì–´ ë³€í™˜ í•„ìˆ˜!)

### RPM í•œì˜ ë³€í™˜
- "ê°œì¸ìš© ìˆ˜ì…" â†’ "personal use importation"
- "ì ˆì°¨" â†’ "procedures process"
- "ê²€ì‚¬ ê±°ë¶€" â†’ "refusal entry detention"
- "relabeling ë¹„ìš©" â†’ "relabeling supervision costs payment"
- "ëˆ„ê°€ ë‚´?" â†’ "who pays costs responsibility"

### GRAS í•œì˜ ë³€í™˜
- "ëŒ€ë‘" â†’ "soy soybean"
- "ìŒë£Œ" â†’ "beverage drink water"

### eCFR í•œì˜ ë³€í™˜
- "ëƒ‰ë™ì‹í’ˆ" â†’ "frozen food"
- "HACCP" â†’ "HACCP hazard analysis"

### DWPE í•œì˜ ë³€í™˜ + ë™ì˜ì–´
- "í•´ì‚°ë¬¼" â†’ "fish fishery seafood shellfish aquatic marine"
- "ì¤‘êµ­" â†’ "China Chinese"

### USC í•œì˜ ë³€í™˜
- "ë¶€ì •í‘œì‹œ" â†’ "misbranding false labeling"
- "ì²˜ë²Œ" â†’ "penalties violations"

## ì¬ì‹œë„ ì „ëµ
ì²« ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ 2-3ë²ˆ ì¬ì‹œë„ í•„ìˆ˜
"""

        # 2. ReAct ì—ì´ì „íŠ¸ ìƒì„± (context ì¶”ê°€)
        self.agent = ReActAgent.from_tools(
            tools=self.fda_tools,
            llm=Settings.llm,
            system_prompt=system_prompt,
            max_iterations=10,
            verbose=True,
            # âœ… í•µì‹¬ ì¶”ê°€: contextë¡œ ë„êµ¬ ê°•ì œ ì‚¬ìš©
            context="""You MUST use tools for FDA-related queries.
NEVER answer with "(Implicit) I can answer without tools".
For keywords like "ë¹„ìš©/cost", "ì ˆì°¨/procedure", "Chapter", "relabeling" â†’ ALWAYS use tools.
Always translate Korean to English before searching."""
        )

    def _is_food_export_question_llm(self, query: str) -> bool:
        """
        ë¹ ë¥´ê³  ì €ë ´í•œ LLM(gpt-3.5-turbo)ì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´
        'íŠ¹ì • ì‹í’ˆì˜ ìˆ˜ì¶œ ê·œì œ'ì— ëŒ€í•œ ê²ƒì¸ì§€ ë¶„ë¥˜í•˜ëŠ” í•„í„° í•¨ìˆ˜.
        """
        try:
            # í•„í„° ì „ìš©ìœ¼ë¡œ ì €ë ´í•œ ëª¨ë¸ì„ ì„ì‹œë¡œ ì‚¬ìš©
            filter_llm = OpenAI(model="gpt-3.5-turbo", temperature=0)
            
            prompt = f"""
            Is the following user query about the regulations for exporting a specific food item?
            Answer ONLY with "Yes" or "No". Do not add any other text, explanation, or punctuation.

            Query: "{query}"
            """
            
            response = filter_llm.complete(prompt)
            answer = response.text.strip().lower()
            
            print(f"LLM Filter Check for query '{query}': Answer='{answer}'") # ë””ë²„ê¹…ìš© ë¡œê·¸
            
            return answer == "yes"

        except Exception as e:
            print(f"LLM Filter failed: {e}") # ì—ëŸ¬ ë¡œê·¸
            return False # ì—ëŸ¬ ë°œìƒ ì‹œ ì•ˆì „í•˜ê²Œ Falseë¡œ ì²˜ë¦¬

    def _decompose_product(self, product_name: str) -> dict:
        """ì œí’ˆ ë¶„í•´ (10ê°œ ìš”ì†Œ) - í•œêµ­ ìŒì‹ ì§€ì› ê°•í™”"""
        # ìºì‹œ í™•ì¸
        if product_name in self.decomposition_cache:
            return self.decomposition_cache[product_name]
        
        # í•œêµ­ì–´ ê°ì§€ ë° ì²˜ë¦¬ ì§€ì¹¨ ì¶”ê°€
        is_korean = any(ord(char) >= 0xAC00 and ord(char) <= 0xD7A3 for char in product_name)
        
        if is_korean:
            prompt_prefix = f"""
You are analyzing a KOREAN food product. First, identify what '{product_name}' is in English.
Common Korean foods:
- ë–¡ë³¶ì´ = Tteokbokki (spicy rice cakes)
- ê¹€ì¹˜ = Kimchi (fermented cabbage)
- ê¹€ë°¥ = Kimbap (rice rolls with vegetables)
- ë§Œë‘ = Mandu (dumplings)
- ë¶ˆê³ ê¸° = Bulgogi (marinated beef)
- ë¹„ë¹”ë°¥ = Bibimbap (mixed rice bowl)
- If not listed above, translate and identify the components.

Now analyze '{product_name}' for FDA requirements.
"""
        else:
            prompt_prefix = f"Analyze '{product_name}' for FDA requirements."
        
        decomposition_prompt = f"""{prompt_prefix}
        
Return a JSON object with EXACTLY these fields:
{{
  "ingredients": [list of main components in English],
  "processes": [manufacturing/cooking methods],
  "allergens": [only FDA major allergens if present: milk, eggs, fish, shellfish, tree nuts, peanuts, wheat, soybeans, sesame],
  "origin": "Korea" if Korean food else appropriate country,
  "category": "ethnic food" if Korean else appropriate category,
  "subcategories": [relevant subcategories],
  "storage_type": "frozen", "refrigerated", or "ambient",
  "risk_level": "high", "medium", or "low",
  "packaging_concerns": [relevant concerns],
  "potential_hazards": [food safety hazards],
  "import_type": "commercial" or "personal use"
}}

Examples:
- For ë–¡ë³¶ì´: {{"ingredients": ["rice cake", "fish cake", "gochujang"], ...}}
- For ê¹€ì¹˜: {{"ingredients": ["cabbage", "chili powder", "garlic"], ...}}

Return ONLY valid JSON, no other text or markdown.
"""
        
        try:
            response = Settings.llm.complete(decomposition_prompt)
            text = response.text.strip()
            
            # Markdown ì½”ë“œ ë¸”ë¡ ì œê±°
            if "```json" in text:
                text = text.split("```json")[1].split("```")[0].strip()
            elif "```" in text:
                text = text.split("```")[1].split("```")[0].strip()
            
            # JSON íŒŒì‹±
            decomposition = json.loads(text)
            
            # í•„ë“œ ê²€ì¦ ë° ê¸°ë³¸ê°’ ì¶”ê°€
            defaults = {
                "ingredients": [],
                "processes": [],
                "allergens": [],
                "origin": "Korea" if is_korean else "unknown",
                "category": "ethnic food" if is_korean else "food",
                "subcategories": [],
                "storage_type": "ambient",
                "risk_level": "medium",
                "packaging_concerns": [],
                "potential_hazards": [],
                "import_type": "commercial"
            }
            
            # ëˆ„ë½ëœ í•„ë“œ ì±„ìš°ê¸°
            for key, default_value in defaults.items():
                if key not in decomposition or not decomposition[key]:
                    decomposition[key] = default_value
            
            # ìºì‹±
            self.decomposition_cache[product_name] = decomposition
            return decomposition
            
        except (json.JSONDecodeError, Exception) as e:
            print(f"Decomposition failed for '{product_name}': {e}")
            print(f"LLM Response: {response.text if 'response' in locals() else 'No response'}")
            
            # ìŠ¤ë§ˆíŠ¸í•œ í´ë°±: LLM í•œ ë²ˆ ë” ì‹œë„ (ë” ê°„ë‹¨í•œ ë°©ì‹)
            try:
                simple_prompt = f"""
What are the main ingredients of {product_name}?
Answer in this exact format:
ingredients: item1, item2, item3
allergens: allergen1, allergen2
"""
                simple_response = Settings.llm.complete(simple_prompt)
                lines = simple_response.text.strip().split('\n')
                
                ingredients = []
                allergens = []
                
                for line in lines:
                    if line.startswith('ingredients:'):
                        ingredients = [i.strip() for i in line.split(':')[1].split(',')]
                    elif line.startswith('allergens:'):
                        allergens = [a.strip() for a in line.split(':')[1].split(',')]
                
                return {
                    "ingredients": ingredients or [product_name],
                    "processes": ["processing", "packaging"],
                    "allergens": allergens,
                    "origin": "Korea" if is_korean else "unknown",
                    "category": "ethnic food" if is_korean else "food",
                    "subcategories": ["imported food"],
                    "storage_type": "refrigerated" if is_korean else "ambient",
                    "risk_level": "medium",
                    "packaging_concerns": ["labeling required"],
                    "potential_hazards": ["contamination"],
                    "import_type": "commercial"
                }
                
            except:
                # ìµœì¢… í´ë°±
                return {
                    "ingredients": [product_name],
                    "processes": [],
                    "allergens": [],
                    "origin": "Korea" if is_korean else "unknown",
                    "category": "ethnic food" if is_korean else "food",
                    "subcategories": [],
                    "storage_type": "ambient",
                    "risk_level": "medium",
                    "packaging_concerns": [],
                    "potential_hazards": [],
                    "import_type": "commercial"
                }

    def _extract_product_name(self, query: str) -> str:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ì—ì„œ ì œí’ˆëª… ì¶”ì¶œ"""
        prompt = f"""
Analyze this user query and determine if it contains a FOOD PRODUCT name.

Query: "{query}"

Rules:
- Return ONLY the food product name if one exists
- Return "None" if this is a general question (about regulations, procedures, concepts)
- Examples of products: "ê¹€ì¹˜", "ìƒˆìš°íŠ€ê¹€", "ëƒ‰ë™ë§Œë‘", "chicken nuggets"
- Examples of NOT products: "HACCP", "FDA", "ê·œì •", "ì ˆì°¨", "ë¼ë²¨ë§"

Answer with ONLY the product name or "None":
"""
        
        try:
            response = Settings.llm.complete(prompt)
            result = response.text.strip()
            
            # "None" ë˜ëŠ” "none" ë°˜í™˜ ì‹œ Noneìœ¼ë¡œ ë³€í™˜
            if result.lower() == "none":
                return None
            
            return result
            
        except Exception as e:
            print(f"LLM product extraction failed: {e}")
            # ì—ëŸ¬ ì‹œ ì•ˆì „í•˜ê²Œ None ë°˜í™˜
            return None

    def _augment_general_query(self, original_query: str) -> str:
        """ì¼ë°˜ ì§ˆë¬¸ì— ëŒ€í•œ LLM ì¿¼ë¦¬ ì¦ê°•"""
        prompt = f"""
ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì„ FDA ê·œì œ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ì— ìµœì í™”ëœ ì˜ì–´ ì¿¼ë¦¬ë¡œ ë³€í™˜í•˜ê³  í™•ì¥í•˜ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: {original_query}

ë‹¤ìŒ ìš”ì†Œë“¤ì„ í¬í•¨í•˜ì—¬ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”:
1. í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì˜ì–´ë¡œ ë³€í™˜
2. ê´€ë ¨ ë™ì˜ì–´ ë° ì „ë¬¸ ìš©ì–´ ì¶”ê°€
3. FDA ê·œì œ ë§¥ë½ì— ë§ëŠ” ê²€ìƒ‰ì–´ í™•ì¥
4. ì»¬ë ‰ì…˜ë³„ íŠ¹í™” í‚¤ì›Œë“œ í¬í•¨

ì˜ˆì‹œ:
- "ë¹„ìš©ì´ ì–¼ë§ˆë‚˜ ë“œë‚˜ìš”?" â†’ "costs payment fees supervision relabeling expenses"
- "ì–´ë–¤ ì ˆì°¨ê°€ í•„ìš”í•œê°€ìš”?" â†’ "procedures process requirements steps documentation"
- "ê·œì •ì€ ë¬´ì—‡ì¸ê°€ìš”?" â†’ "regulations requirements CFR guidelines compliance"

ë³€í™˜ëœ ê²€ìƒ‰ ì¿¼ë¦¬ë§Œ ë°˜í™˜í•˜ì„¸ìš” (ì„¤ëª… ì—†ì´):
"""
        
        try:
            response = Settings.llm.complete(prompt)
            augmented_query = response.text.strip()
            
            # ì›ë³¸ ì¿¼ë¦¬ì™€ ì¦ê°•ëœ ì¿¼ë¦¬ ê²°í•©
            return f"{original_query}\n\nEnhanced search query: {augmented_query}"
            
        except Exception as e:
            print(f"Query augmentation failed: {e}")
            return original_query

    def _augment_query(self, original_query: str, decomposition: dict) -> str:
        """ë¶„í•´ëœ 10ê°œ ìš”ì†Œë¥¼ ëª¨ë‘ í™œìš©í•˜ëŠ” ì¿¼ë¦¬ ì¦ê°•"""
        augmented = f"""
User Question: {original_query}

PRODUCT ANALYSIS (10 elements):
1. Ingredients: {', '.join(decomposition.get('ingredients', []))}
2. Processes: {', '.join(decomposition.get('processes', []))}  
3. Allergens: {', '.join(decomposition.get('allergens', []))}
4. Origin: {decomposition.get('origin', 'unknown')}
5. Category: {decomposition.get('category', 'food')}
6. Subcategories: {', '.join(decomposition.get('subcategories', []))}
7. Storage: {decomposition.get('storage_type', 'ambient')}
8. Risk Level: {decomposition.get('risk_level', 'medium')}
9. Packaging Concerns: {', '.join(decomposition.get('packaging_concerns', []))}
10. Potential Hazards: {', '.join(decomposition.get('potential_hazards', []))}

SEARCH STRATEGY (7 collections):
1. guidance: ì‹¤ë¬´ ê°€ì´ë“œ (CPG, ë¼ë²¨ë§, ì•Œë ˆë¥´ê¸°)
2. ecfr: êµ¬ì²´ì  ê·œì • (21 CFR)
3. gras: ì¬ë£Œ ì•ˆì „ì„± í™•ì¸
4. dwpe: Import Alert í™•ì¸
5. fsvp: ìˆ˜ì…ì ê²€ì¦ ì˜ë¬´
6. rpm: ìˆ˜ì… ìš´ì˜ ì ˆì°¨
7. usc: ë²•ì  ê¸°ë°˜ (21 USC)

Use the most relevant collections based on the product characteristics above.
"""
        return augmented

    def _extract_citations_from_response(self, response) -> dict:
        """Extract citations (title/url) from LlamaIndex response source nodes."""
        citations = []
        sources = []
        keywords = []
        try:
            if hasattr(response, 'source_nodes') and response.source_nodes:
                for node in response.source_nodes:
                    meta = getattr(node, 'metadata', {}) or {}
                    title = meta.get('title') or meta.get('document_title') or meta.get('collection') or 'Reference'
                    url = meta.get('url') or meta.get('source') or meta.get('link')
                    description = meta.get('summary') or meta.get('description') or 'ê´€ë ¨ ê·œì •/ìë£Œ'
                    if url:
                        item = {"title": title, "description": description, "url": url}
                        if item not in citations:
                            citations.append(item)
                            sources.append(title)
                    # derive simple keywords from tool/collection
                    tool_name = meta.get('tool_name') or meta.get('collection')
                    if tool_name:
                        keywords.append(tool_name)
        except Exception:
            pass
        return {"cfr_references": citations, "sources": sources, "keywords": list(dict.fromkeys(keywords))}

    def _format_parallel_results(self, results: List[Dict]) -> str:
        """ë³‘ë ¬ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬ë§·"""
        if not results:
            return "ë³‘ë ¬ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
        
        formatted = []
        for i, result in enumerate(results[:10], 1):  # 5 â†’ 10ê°œë¡œ ì¦ê°€
            formatted.append(f"""
{i}. [{result['score']:.2f}] {result['collection']} {result.get('collection_role', '')}
   ì œëª©: {result.get('title', 'N/A')}
   ë‚´ìš©: {result.get('text', 'N/A')[:800]}...  # 200 â†’ 800ìë¡œ ì¦ê°€
   ì¶œì²˜: {result.get('collection_desc', '')}
""")
        
        return "\n".join(formatted)

    def chat(self, query: str) -> dict:
        """ì‚¬ìš©ì ì œì•ˆ êµ¬ì¡°: ì œí’ˆ ì§ˆë¬¸ì€ ë¶„í•´, ì¼ë°˜ ì§ˆë¬¸ì€ LLM ì¦ê°•"""
        try:
            product = self._extract_product_name(query)
            
            if product:
                # ì œí’ˆ ì§ˆë¬¸: ë¶„í•´ ë°©ì‹
                print(f"ğŸ“¦ ì œí’ˆ ì§ˆë¬¸ ê°ì§€: {product}")
                decomposition = self._decompose_product(product)
                search_query = query  # ì›ë³¸ ì‚¬ìš©
                print(f"ğŸ”¬ ì œí’ˆ ë¶„í•´ ì™„ë£Œ: {decomposition.get('category')}")
            else:
                # ì¼ë°˜ ì§ˆë¬¸: LLM ì¦ê°• ë°©ì‹
                print("ğŸ” ì¼ë°˜ ì§ˆë¬¸ ê°ì§€ - LLM ì¦ê°• ì ìš©")
                decomposition = None
                search_query = self._augment_general_query(query)  # ì—¬ê¸°ì„œ ì¦ê°•!
                print(f"âœ¨ ì¦ê°•ëœ ì¿¼ë¦¬: {search_query[:100]}...")
            
            # orchestratorì— ì „ë‹¬ (ìˆœìˆ˜ ê²€ìƒ‰ë§Œ ë‹´ë‹¹)
            from utils.orchestrator import SimpleOrchestrator
            orchestrator = SimpleOrchestrator()
            
            if decomposition:
                # ì œí’ˆ ì§ˆë¬¸: ë¶„í•´ ê¸°ë°˜ ì»¬ë ‰ì…˜ ì„ íƒ
                collections = orchestrator.determine_collections(decomposition)
            else:
                # ì¼ë°˜ ì§ˆë¬¸: ê¸°ë³¸ ì»¬ë ‰ì…˜ ì‚¬ìš©
                collections = ['guidance', 'ecfr', 'gras', 'dwpe', 'fsvp', 'rpm', 'usc']
            
            print(f"ğŸ“š ê²€ìƒ‰í•  ì»¬ë ‰ì…˜: {collections}")
            
            # ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰
            parallel_results = orchestrator.parallel_search(
                query=search_query,  # ì¦ê°•ëœ ë˜ëŠ” ì›ë³¸
                collections=collections,
                decomposition=decomposition
            )
            
            ranked_results = orchestrator.merge_and_rank(parallel_results)
            print(f"âš¡ ë³‘ë ¬ ê²€ìƒ‰ ì™„ë£Œ: {parallel_results['search_time']:.2f}ì´ˆ, {len(ranked_results)}ê°œ ê²°ê³¼")
            
            # ê²°ê³¼ ì¶©ë¶„ì„± í‰ê°€ ë° ì‘ë‹µ ìƒì„±
            if self._is_parallel_result_sufficient(ranked_results, decomposition or {}):
                # decomposition ìˆë“  ì—†ë“ , ì¶©ë¶„í•˜ë©´ ì§ì ‘ ë‹µë³€
                print("âœ… ë³‘ë ¬ ê²€ìƒ‰ ê²°ê³¼ë§Œìœ¼ë¡œ ì¶©ë¶„ - ì§ì ‘ ë‹µë³€ ìƒì„±")
                return self._generate_direct_response(query, ranked_results, decomposition)
            else:
                # ReAct Agentë¡œ ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘
                print("ğŸ”„ ReAct Agentë¡œ ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘")
                search_summary = self._format_parallel_results(ranked_results)
                
                if decomposition:
                    enhanced_query = f"""
{self._augment_query(query, decomposition)}

## ê²€ìƒ‰ëœ FDA ë¬¸ì„œë“¤
{search_summary}

ìœ„ ì •ë³´ë¥¼ í™œìš©í•˜ê³ , ë¶€ì¡±í•œ ë¶€ë¶„ë§Œ ì¶”ê°€ ê²€ìƒ‰í•˜ì„¸ìš”.
ì •ë³´ ìˆ˜ì§‘ë§Œ í•˜ê³ , ìµœì¢… ë‹µë³€ì€ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.
"""
                else:
                    enhanced_query = f"""
{search_query}

## ê²€ìƒ‰ëœ FDA ë¬¸ì„œë“¤
{search_summary}

ìœ„ ì •ë³´ë¥¼ í™œìš©í•˜ê³ , ë¶€ì¡±í•œ ë¶€ë¶„ë§Œ ì¶”ê°€ ê²€ìƒ‰í•˜ì„¸ìš”.
ì •ë³´ ìˆ˜ì§‘ë§Œ í•˜ê³ , ìµœì¢… ë‹µë³€ì€ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.
"""
                
                context = self.memory.get_context_for_agent()
                full_query = f"{context}\n{enhanced_query}" if context else enhanced_query
                
                # Agentë¡œ ì •ë³´ ìˆ˜ì§‘ë§Œ
                print("ğŸ” Agent ì •ë³´ ìˆ˜ì§‘ ì‹œì‘...")
                agent_response = self.agent.chat(full_query)
                collected_info = str(agent_response)
                
                # ë³‘ë ¬ ê²€ìƒ‰ + Agent ì •ë³´ë¥¼ í•©ì³ì„œ ìµœì¢… ë‹µë³€ ìƒì„±
                print("âœ… ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ - ìµœì¢… ë‹µë³€ ìƒì„±")
                return self._generate_response_with_agent_info(
                    query=query,
                    parallel_results=ranked_results,
                    agent_info=collected_info,
                    decomposition=decomposition
                )
            
        except Exception as e:
            print(f"Error in chat: {e}")
            fallback = self._generate_fallback_response(query)
            return {
                "content": fallback,
                "cfr_references": [],
                "sources": [],
                "keywords": []
            }

    def _is_parallel_result_sufficient(self, results: List[Dict], decomposition: dict) -> bool:
        """ë³‘ë ¬ ê²€ìƒ‰ ê²°ê³¼ì˜ ì¶©ë¶„ì„± í‰ê°€ (ì œí’ˆ ì§ˆë¬¸ê³¼ ì¼ë°˜ ì§ˆë¬¸ ëª¨ë‘ ì§€ì›)"""
        print(f"\nğŸ” ì¶©ë¶„ì„± í‰ê°€ ì‹œì‘")
        print(f"  - ì „ì²´ ê²°ê³¼ ê°œìˆ˜: {len(results)}")
        
        # ê¸°ë³¸ ì¡°ê±´ ì²´í¬
        if not results or len(results) < 5:
            print(f"  âŒ ê²°ê³¼ ë¶€ì¡±: {len(results)}ê°œ")
            return False
        
        # GRAS í•„í„°ë§ - ì¼ë‹¨ ë¹„í™œì„±í™”
        # filtered_results = self._filter_gras_noise(results, decomposition)
        filtered_results = results  # ê·¸ëŒ€ë¡œ ì‚¬ìš©
        
        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        avg_score = sum(r['score'] for r in filtered_results) / len(filtered_results)
        print(f"  - í‰ê·  ì ìˆ˜: {avg_score:.3f} (ì„ê³„ê°’: 0.60)")
        if avg_score < 0.60:
            print(f"  âŒ í‰ê·  ì ìˆ˜ ë¶€ì¡±")
            return False
        
        # ì»¬ë ‰ì…˜ ë‹¤ì–‘ì„± ì²´í¬ (ìµœì†Œ 3ê°œ ì´ìƒì˜ ì»¬ë ‰ì…˜ì—ì„œ ê²°ê³¼)
        unique_collections = set(r['collection'] for r in filtered_results)
        print(f"  - ì»¬ë ‰ì…˜ ë‹¤ì–‘ì„±: {len(unique_collections)}ê°œ {list(unique_collections)}")
        if len(unique_collections) < 3:
            print(f"  âŒ ì»¬ë ‰ì…˜ ë‹¤ì–‘ì„± ë¶€ì¡±")
            return False
        
        # í•µì‹¬ ì»¬ë ‰ì…˜ í¬í•¨ ì—¬ë¶€ ì²´í¬ (ì¼ë°˜ ì§ˆë¬¸ë„ ë™ì¼í•œ ê¸°ì¤€ ì ìš©)
        essential_collections = {'guidance', 'ecfr', 'gras'}  # ë¼ë²¨ë§, ê·œì •, ì•ˆì „ì„±
        has_essential = [c for c in essential_collections if c in unique_collections]
        print(f"  - í•„ìˆ˜ ì»¬ë ‰ì…˜: {has_essential}")
        if not has_essential:
            print(f"  âŒ í•„ìˆ˜ ì»¬ë ‰ì…˜ ì—†ìŒ")
            return False
        
        # ì•Œë ˆë¥´ê¸° ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš° guidance ê²°ê³¼ í•„ìˆ˜ (ì œí’ˆ ì§ˆë¬¸ë§Œ)
        # if decomposition and decomposition.get('allergens') and 'guidance' not in unique_collections:
        #     return False
        
        print(f"  âœ… ì¶©ë¶„ì„± í‰ê°€ í†µê³¼!\n")
        return True

    def _generate_direct_response(self, query: str, results: List[Dict], decomposition: dict) -> dict:
        """ë³‘ë ¬ ê²€ìƒ‰ ê²°ê³¼ë§Œìœ¼ë¡œ ì§ì ‘ ë‹µë³€ ìƒì„± (ì œí’ˆ ì§ˆë¬¸ê³¼ ì¼ë°˜ ì§ˆë¬¸ ëª¨ë‘ ì§€ì›)"""
        
        # ì¶œì²˜ ë²ˆí˜¸ ë§¤í•‘ ìƒì„±
        citations = []
        for i, r in enumerate(results[:10], 1):
            # ì œëª©ì´ ë¹„ì–´ìˆê±°ë‚˜ Noneì¸ ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •
            title = r.get('title', '').strip()
            if not title:
                title = f"{r['collection'].upper()} Document {i}"
            
            # URLì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ê¸°ë³¸ URL ìƒì„±
            url = r.get('url', '').strip()
            if not url:
                # ì»¬ë ‰ì…˜ë³„ ê¸°ë³¸ URL ìƒì„±
                if r['collection'] == 'fsvp':
                    url = f"https://www.fda.gov/food/importing-food-products-united-states/foreign-suppliers-verification-programs-fsvp-importer-portal-records-submission"
                elif r['collection'] == 'gras':
                    url = f"https://www.hfpappexternal.fda.gov/scripts/fdcc/index.cfm?set=GRASNotices"
                elif r['collection'] == 'ecfr':
                    url = f"https://www.ecfr.gov/current/title-21"
                elif r['collection'] == 'guidance':
                    url = f"https://www.fda.gov/regulatory-information/search-fda-guidance-documents"
                elif r['collection'] == 'dwpe':
                    url = f"https://www.accessdata.fda.gov/cms_ia/country_KR.html"
                elif r['collection'] == 'usc':
                    url = f"https://www.law.cornell.edu/uscode/text/21"
            
            citations.append({
                "index": i,
                "collection": r['collection'],
                "title": title,
                "url": url,
                "score": r['score']
            })
        
        # ì¶œì²˜ ë¦¬ìŠ¤íŠ¸ (í”„ë¡¬í”„íŠ¸ìš©)
        source_list = "\n".join([
            f"[ì¶œì²˜ {c['index']}] {c['collection']}: {c['title'][:80]}"
            for c in citations
        ])
        
        # ì „ì²´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í’ë¶€í•˜ê²Œ ì „ë‹¬
        full_context = "\n\n".join([
            f"[ì¶œì²˜ {i+1}] {r['collection'].upper()} (ì ìˆ˜: {r['score']:.3f})\n"
            f"ì œëª©: {r.get('title', 'N/A')}\n"
            f"ë‚´ìš©: {r.get('text', '')[:800]}"
            for i, r in enumerate(results[:10])
        ])
        
        if decomposition:
            prompt = f"""
ì‚¬ìš©ì ì§ˆë¬¸: {query}

ì œí’ˆ íŠ¹ì„±:
{json.dumps(decomposition, indent=2, ensure_ascii=False)}

ğŸ“– ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ (ê° ë‚´ìš© ì•ì˜ [ì¶œì²˜ N]ì„ ë³´ê³  ì£¼ì„ì„ ë‹¬ì•„ì•¼ í•¨):
{full_context}

## ì¶œì²˜ ëª©ë¡
{source_list}

ìœ„ ë¬¸ì„œë“¤ì„ ì¢…í•©í•˜ì—¬ ë‹¤ìŒ ì‚¬í•­ì„ í¬í•¨í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”:
1. êµ¬ì²´ì ì¸ CFR ê·œì • ë²ˆí˜¸ì™€ ë‚´ìš©
2. Import Alert ì—¬ë¶€
3. ì•Œë ˆë¥´ê¸° ë¼ë²¨ë§ êµ¬ì²´ì  ìš”êµ¬ì‚¬í•­
4. FSVP ê²€ì¦ ì ˆì°¨
5. ì‹¤ë¬´ ì²´í¬ë¦¬ìŠ¤íŠ¸

â—ï¸í•µì‹¬ ê·œì¹™:
- ì¤‘ìš”í•œ ì •ë³´ë‚˜ ê·œì •ì„ ì–¸ê¸‰í•  ë•Œë§ˆë‹¤ í•´ë‹¹í•˜ëŠ” ì¶œì²˜ ë²ˆí˜¸ë¥¼ [1], [2] í˜•íƒœë¡œ ë¬¸ì¥ ëì— ì‚½ì…í•˜ì„¸ìš”.
- ì—¬ëŸ¬ ì¶œì²˜ë¥¼ ì°¸ê³ í•œ ê²½ìš° [1][2] ì²˜ëŸ¼ ì—°ì†ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”.
- ë°˜ë“œì‹œ [ì¶œì²˜ N] ì •ë³´ë¥¼ í™•ì¸í•˜ê³  ì •í™•í•œ ë²ˆí˜¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

ì˜ˆì‹œ:
- ìƒˆìš°ëŠ” ì£¼ìš” ì•Œë ˆë¥´ê¸° ìœ ë°œ ë¬¼ì§ˆë¡œ í‘œì‹œí•´ì•¼ í•©ë‹ˆë‹¤[1].
- 21 CFR 1250.26ê³¼ Import Alert 16-50ì„ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤[2][3].

í•œêµ­ì–´ë¡œ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
"""
        else:
            prompt = f"""
ì‚¬ìš©ì ì§ˆë¬¸: {query}

ğŸ“– ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ (ê° ë‚´ìš© ì•ì˜ [ì¶œì²˜ N]ì„ ë³´ê³  ì£¼ì„ì„ ë‹¬ì•„ì•¼ í•¨):
{full_context}

## ì¶œì²˜ ëª©ë¡
{source_list}

ìœ„ ë¬¸ì„œë“¤ì„ ì¢…í•©í•˜ì—¬ ë‹µë³€í•˜ë˜, **ê° ì£¼ì¥ ë’¤ì— [1], [2] í˜•ì‹ìœ¼ë¡œ ì¶œì²˜ ë²ˆí˜¸ë¥¼ í‘œì‹œ**í•˜ì„¸ìš”.

â—ï¸í•µì‹¬ ê·œì¹™:
- ì¤‘ìš”í•œ ì •ë³´ë‚˜ ê·œì •ì„ ì–¸ê¸‰í•  ë•Œë§ˆë‹¤ í•´ë‹¹í•˜ëŠ” ì¶œì²˜ ë²ˆí˜¸ë¥¼ [1], [2] í˜•íƒœë¡œ ë¬¸ì¥ ëì— ì‚½ì…í•˜ì„¸ìš”.
- ì—¬ëŸ¬ ì¶œì²˜ë¥¼ ì°¸ê³ í•œ ê²½ìš° [1][2] ì²˜ëŸ¼ ì—°ì†ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”.

ì˜ˆì‹œ:
- FDAëŠ” ì‹í’ˆ ì•Œë ˆë¥´ê¸° í‘œì‹œë¥¼ ì˜ë¬´í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤[1].
- 21 CFR ê·œì •ì„ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤[2][3].

í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
"""
        
        response = Settings.llm.complete(prompt)
        
        print(f"\nğŸ“‹ Citations ìƒì„± ì™„ë£Œ:")
        print(f"  - ì´ {len(citations)}ê°œ citations ìƒì„±")
        for c in citations:
            print(f"    [{c['index']}] {c['collection']}: {c['title'][:50]}...")
        
        return {
            "content": response.text,
            "citations": citations,
            "cfr_references": [],
            "sources": [c['title'] for c in citations[:5]],
            "keywords": list(set(r['collection'] for r in results))
        }

    def _generate_response_with_agent_info(
        self, 
        query: str, 
        parallel_results: List[Dict],
        agent_info: str,
        decomposition: dict
    ) -> dict:
        """ë³‘ë ¬ ê²€ìƒ‰ + Agent ìˆ˜ì§‘ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ë‹µë³€ ìƒì„±"""
        
        print("\n" + "="*60)
        print("ğŸ“ ìµœì¢… ë‹µë³€ ìƒì„± ì‹œì‘")
        print("="*60)
        
        # ì¶œì²˜ ë²ˆí˜¸ ë§¤í•‘ ìƒì„±
        citations = []
        for i, r in enumerate(parallel_results[:10], 1):
            # ì œëª©ì´ ë¹„ì–´ìˆê±°ë‚˜ Noneì¸ ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •
            title = r.get('title', '').strip()
            if not title:
                title = f"{r['collection'].upper()} Document {i}"
            
            # URLì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ê¸°ë³¸ URL ìƒì„±
            url = r.get('url', '').strip()
            if not url:
                # ì»¬ë ‰ì…˜ë³„ ê¸°ë³¸ URL ìƒì„±
                if r['collection'] == 'fsvp':
                    url = f"https://www.fda.gov/food/importing-food-products-united-states/foreign-suppliers-verification-programs-fsvp-importer-portal-records-submission"
                elif r['collection'] == 'gras':
                    url = f"https://www.hfpappexternal.fda.gov/scripts/fdcc/index.cfm?set=GRASNotices"
                elif r['collection'] == 'ecfr':
                    url = f"https://www.ecfr.gov/current/title-21"
                elif r['collection'] == 'guidance':
                    url = f"https://www.fda.gov/regulatory-information/search-fda-guidance-documents"
                elif r['collection'] == 'dwpe':
                    url = f"https://www.fda.gov/import-alerts"
                elif r['collection'] == 'usc':
                    url = f"https://www.law.cornell.edu/uscode/text/21"
            
            citations.append({
                "index": i,
                "collection": r['collection'],
                "title": title,
                "url": url,
                "score": r['score']
            })
        
        # ì¶œì²˜ ë¦¬ìŠ¤íŠ¸ (í”„ë¡¬í”„íŠ¸ìš©)
        source_list = "\n".join([
            f"[ì¶œì²˜ {c['index']}] {c['collection']}: {c['title'][:80]}"
            for c in citations
        ])
        
        # ë³‘ë ¬ ê²€ìƒ‰ ê²°ê³¼ ì •ë¦¬ (Streamlit ìŠ¤íƒ€ì¼)
        parallel_context = "\n\n".join([
            f"[ì¶œì²˜ {i+1}] {r['collection'].upper()} (ì ìˆ˜: {r['score']:.3f})\n"
            f"ì œëª©: {r.get('title', 'N/A')}\n"
            f"ë‚´ìš©: {r.get('text', '')[:800]}"
            for i, r in enumerate(parallel_results[:10])
        ])
        
        print(f"ğŸ“Š ì…ë ¥ ì •ë³´:")
        print(f"  - ë³‘ë ¬ ê²€ìƒ‰ ê²°ê³¼: {len(parallel_results)}ê°œ")
        print(f"  - Agent ìˆ˜ì§‘ ì •ë³´: {len(agent_info)}ì")
        print(f"  - ì´ ì»¨í…ìŠ¤íŠ¸: {len(parallel_context) + len(agent_info)}ì")
        
        # í†µí•© í”„ë¡¬í”„íŠ¸
        if decomposition:
            prompt = f"""
ì‚¬ìš©ì ì§ˆë¬¸: {query}

ì œí’ˆ íŠ¹ì„±:
{json.dumps(decomposition, indent=2, ensure_ascii=False)}

ğŸ“– ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ (ê° ë‚´ìš© ì•ì˜ [ì¶œì²˜ N]ì„ ë³´ê³  ì£¼ì„ì„ ë‹¬ì•„ì•¼ í•¨):
{parallel_context}

Agentê°€ ì¶”ê°€ ìˆ˜ì§‘í•œ ì •ë³´:
{agent_info}

## ì¶œì²˜ ëª©ë¡
{source_list}

ìœ„ ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ë‹¤ìŒì„ í¬í•¨í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”:
1. êµ¬ì²´ì ì¸ CFR ê·œì • ë²ˆí˜¸ì™€ ë‚´ìš©
2. Import Alert ì—¬ë¶€
3. ì•Œë ˆë¥´ê¸° ë¼ë²¨ë§ êµ¬ì²´ì  ìš”êµ¬ì‚¬í•­
4. FSVP ê²€ì¦ ì ˆì°¨
5. ì‹¤ë¬´ ì²´í¬ë¦¬ìŠ¤íŠ¸ (5ê°œ ì´ìƒ)

â—ï¸í•µì‹¬ ê·œì¹™:
- ì¤‘ìš”í•œ ì •ë³´ë‚˜ ê·œì •ì„ ì–¸ê¸‰í•  ë•Œë§ˆë‹¤ í•´ë‹¹í•˜ëŠ” ì¶œì²˜ ë²ˆí˜¸ë¥¼ [1], [2] í˜•íƒœë¡œ ë¬¸ì¥ ëì— ì‚½ì…í•˜ì„¸ìš”.
- ì—¬ëŸ¬ ì¶œì²˜ë¥¼ ì°¸ê³ í•œ ê²½ìš° [1][2] ì²˜ëŸ¼ ì—°ì†ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”.
- ë°˜ë“œì‹œ [ì¶œì²˜ N] ì •ë³´ë¥¼ í™•ì¸í•˜ê³  ì •í™•í•œ ë²ˆí˜¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

ì˜ˆì‹œ:
- ìƒˆìš°ëŠ” ì£¼ìš” ì•Œë ˆë¥´ê¸° ìœ ë°œ ë¬¼ì§ˆë¡œ í‘œì‹œí•´ì•¼ í•©ë‹ˆë‹¤[1].
- 21 CFR 1250.26ê³¼ Import Alert 16-50ì„ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤[2][3].

í•œêµ­ì–´ë¡œ 500ë‹¨ì–´ ì´ìƒ, êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
"""
        else:
            prompt = f"""
ì‚¬ìš©ì ì§ˆë¬¸: {query}

ğŸ“– ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ (ê° ë‚´ìš© ì•ì˜ [ì¶œì²˜ N]ì„ ë³´ê³  ì£¼ì„ì„ ë‹¬ì•„ì•¼ í•¨):
{parallel_context}

Agentê°€ ì¶”ê°€ ìˆ˜ì§‘í•œ ì •ë³´:
{agent_info}

## ì¶œì²˜ ëª©ë¡
{source_list}

ìœ„ ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ë‹µë³€í•˜ë˜, **ê° ì£¼ì¥ ë’¤ì— [1], [2] í˜•ì‹ìœ¼ë¡œ ì¶œì²˜ ë²ˆí˜¸ë¥¼ í‘œì‹œ**í•˜ì„¸ìš”.

â—ï¸í•µì‹¬ ê·œì¹™:
- ì¤‘ìš”í•œ ì •ë³´ë‚˜ ê·œì •ì„ ì–¸ê¸‰í•  ë•Œë§ˆë‹¤ í•´ë‹¹í•˜ëŠ” ì¶œì²˜ ë²ˆí˜¸ë¥¼ [1], [2] í˜•íƒœë¡œ ë¬¸ì¥ ëì— ì‚½ì…í•˜ì„¸ìš”.
- ì—¬ëŸ¬ ì¶œì²˜ë¥¼ ì°¸ê³ í•œ ê²½ìš° [1][2] ì²˜ëŸ¼ ì—°ì†ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”.

ì˜ˆì‹œ:
- FDAëŠ” ì‹í’ˆ ì•Œë ˆë¥´ê¸° í‘œì‹œë¥¼ ì˜ë¬´í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤[1].
- 21 CFR ê·œì •ì„ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤[2][3].

í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
"""
        
        print(f"\nğŸ¤– LLM í˜¸ì¶œ ì¤‘... (í”„ë¡¬í”„íŠ¸: {len(prompt)}ì)")
        
        # ë‹¨ì¼ LLM í˜¸ì¶œë¡œ ìµœì¢… ë‹µë³€ ìƒì„±
        response = Settings.llm.complete(prompt)
        
        print(f"\nâœ… ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ!")
        print(f"  - ë‹µë³€ ê¸¸ì´: {len(response.text)}ì")
        print(f"  - ë‹µë³€ ë‹¨ì–´ ìˆ˜: {len(response.text.split())}ë‹¨ì–´")
        
        print(f"\nğŸ“‹ Citations ìƒì„± ì™„ë£Œ:")
        print(f"  - ì´ {len(citations)}ê°œ citations ìƒì„±")
        for c in citations:
            print(f"    [{c['index']}] {c['collection']}: {c['title'][:50]}...")
        
        # ìµœì¢… ë‹µë³€ ë‚´ìš© ì¶œë ¥
        print("\n" + "="*60)
        print("ğŸ“„ ìµœì¢… ë‹µë³€ ë‚´ìš©:")
        print("="*60)
        print(response.text)
        print("="*60 + "\n")
        
        return {
            "content": response.text,
            "citations": citations,
            "cfr_references": [],
            "sources": [c['title'] for c in citations[:5]],
            "keywords": list(set(r['collection'] for r in parallel_results))
        }

    def _generate_fallback_response(self, query: str) -> str:
        """ê²€ìƒ‰ ì‹¤íŒ¨ì‹œ í´ë°± ì‘ë‹µ"""
        return f"""
ì£„ì†¡í•©ë‹ˆë‹¤. '{query}'ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ì¼ë°˜ì ìœ¼ë¡œ ì‹í’ˆ ìˆ˜ì¶œ ì‹œ í™•ì¸í•´ì•¼ í•  FDA ê·œì œ ì‚¬í•­:

1. **ì œì¡° ì‹œì„¤ ìš”êµ¬ì‚¬í•­**
   - FDA ì‹œì„¤ ë“±ë¡ (Food Facility Registration)
   - HACCP ë˜ëŠ” HARPC ê³„íš ìˆ˜ë¦½

2. **ë¼ë²¨ë§ ê·œì •**
   - ì˜ì–‘ì„±ë¶„í‘œ (Nutrition Facts)
   - ì›ì¬ë£Œ ëª©ë¡ (Ingredient List)
   - ì•Œë ˆë¥´ê¸° ìœ ë°œ ë¬¼ì§ˆ í‘œì‹œ

3. **ì‹í’ˆ ì•ˆì „ ê¸°ì¤€**
   - ë¯¸ìƒë¬¼ í•œê³„ ê¸°ì¤€ ì¤€ìˆ˜
   - ì”ë¥˜ ë†ì•½ ë° ì¤‘ê¸ˆì† ê¸°ì¤€

ë” êµ¬ì²´ì ì¸ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ FDA ê³µì‹ ì›¹ì‚¬ì´íŠ¸ë‚˜ ì „ë¬¸ ì»¨ì„¤í„´íŠ¸ ìƒë‹´ì„ ê¶Œì¥í•©ë‹ˆë‹¤.
"""

    def _extract_used_tools(self, response) -> List[str]:
        """ì‘ë‹µì—ì„œ ì‚¬ìš©ëœ íˆ´ ëª©ë¡ì„ ì¶”ì¶œ"""
        used_tools = []
        if hasattr(response, 'source_nodes'):
            for node in response.source_nodes:
                if hasattr(node, 'metadata') and 'tool_name' in node.metadata:
                    tool_name = node.metadata['tool_name']
                    if tool_name not in used_tools:
                        used_tools.append(tool_name)
        return used_tools
    
    def reset_conversation(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.memory.clear_history()
        # ì—ì´ì „íŠ¸ë„ ìƒˆë¡œ ì‹œì‘
        self.agent.reset()


    ## í˜„ì¬ ì‚¬ìš©ë˜ì§€ ì•Šì•„ì„œ ìˆ˜ì •í•˜ì§€ ì•ŠìŒ. 
    def stream_chat(self, query: str):
        """
        ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤. (í–¥í›„ í™•ì¥ ê¸°ëŠ¥)
        """
        response_stream = self.agent.stream_chat(query)
        for token in response_stream.response_gen:
            yield token
