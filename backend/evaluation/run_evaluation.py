# evaluation/run_evaluation.py
"""
í‰ê°€ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
sys.path.append('..')

from evaluation.test_dataset import get_dataset
from evaluation.evaluator import FDAEvaluator
from utils.agent import FDAAgent
from datetime import datetime


def run_evaluation(version_name: str = "baseline"):
    """í‰ê°€ ì‹¤í–‰"""
    
    print("="*80)
    print(f"ğŸ§ª FDA RAG ì‹œìŠ¤í…œ í‰ê°€ - {version_name}")
    print("="*80)
    
    # Agent ì´ˆê¸°í™”
    agent = FDAAgent()
    evaluator = FDAEvaluator()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ
    test_dataset = get_dataset()
    
    print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(test_dataset)}ê°œ")
    print(f"ì¹´í…Œê³ ë¦¬: {set(t['category'] for t in test_dataset)}")
    print()
    
    # ê° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    for i, test_case in enumerate(test_dataset, 1):
        print(f"\n{'='*80}")
        print(f"[{i}/{len(test_dataset)}] {test_case['id']}: {test_case['question'][:50]}...")
        print(f"{'='*80}")
        
        try:
            # Agent í˜¸ì¶œ
            response = agent.chat(test_case['question'])
            
            # ê²€ìƒ‰ ë¬¸ì„œ ì¶”ì¶œ (citationsì—ì„œ)
            retrieved_docs = []
            if 'citations' in response:
                for citation in response['citations']:
                    retrieved_docs.append({
                        'collection': citation.get('collection', ''),
                        'title': citation.get('title', ''),
                        'score': citation.get('score', 0),
                        'text': ''  # ì „ì²´ í…ìŠ¤íŠ¸ëŠ” ì—†ì§€ë§Œ í‰ê°€ ê°€ëŠ¥
                    })
            
            # í‰ê°€
            result = evaluator.evaluate_single(
                test_case=test_case,
                agent_response=response,
                retrieved_docs=retrieved_docs
            )
            
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{version_name}_{timestamp}.json"
    
    filepath = evaluator.save_report(filename)
    
    # ìš”ì•½ ì¶œë ¥
    report = evaluator.generate_report()
    
    print("\n" + "="*80)
    print("ğŸ“Š í‰ê°€ ì™„ë£Œ - ê²°ê³¼ ìš”ì•½")
    print("="*80)
    
    overall = report['overall_metrics']
    print(f"\nì „ì²´ í‰ê· :")
    print(f"  - Correctness: {overall['correctness']:.3f}")
    print(f"  - Faithfulness: {overall['faithfulness']:.3f}")
    print(f"  - Relevancy: {overall['relevancy']:.3f}")
    print(f"  - Similarity: {overall['similarity']:.3f}")
    print(f"  - Keyword Coverage: {overall['keyword_coverage']:.3f}")
    
    print(f"\nì¹´í…Œê³ ë¦¬ë³„:")
    for cat, metrics in report['by_category'].items():
        print(f"  {cat}:")
        print(f"    - Count: {metrics['count']}")
        print(f"    - Correctness: {metrics['correctness']:.3f}")
        print(f"    - Faithfulness: {metrics['faithfulness']:.3f}")
    
    print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼: {filepath}")
    
    return report


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='FDA RAG ì‹œìŠ¤í…œ í‰ê°€')
    parser.add_argument(
        '--version',
        type=str,
        default='baseline',
        help='ë²„ì „ ì´ë¦„ (ì˜ˆ: baseline, with_reranking)'
    )
    
    args = parser.parse_args()
    
    run_evaluation(args.version)