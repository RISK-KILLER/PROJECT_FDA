# evaluation/run_evaluation.py
"""
í‰ê°€ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
sys.path.append('..')

from evaluation.test_dataset import get_dataset
from evaluation.evaluator import FDAEvaluator
from utils.agent import FDAAgent
from datetime import datetime

# â­ í‰ê°€ìš© ì„¤ì •
import os
from dotenv import load_dotenv
from llama_index.core import Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

load_dotenv()


def run_evaluation(version_name: str = "baseline", deterministic: bool = True):
    """í‰ê°€ ì‹¤í–‰
    
    Args:
        version_name: ë²„ì „ ì´ë¦„
        deterministic: Trueì´ë©´ temperature=0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì¼ê´€ëœ ê²°ê³¼ ë³´ì¥
    """
    
    print("="*80)
    print(f"ğŸ§ª FDA RAG ì‹œìŠ¤í…œ í‰ê°€ - {version_name}")
    print("="*80)
    
    # â­ í‰ê°€ìš©ìœ¼ë¡œ temperatureë¥¼ 0ìœ¼ë¡œ ì„¤ì • (ì¼ê´€ëœ ê²°ê³¼ë¥¼ ìœ„í•´)
    if deterministic:
        print("ğŸ”§ í‰ê°€ ëª¨ë“œ: temperature=0 (ì¼ê´€ëœ ê²°ê³¼ ë³´ì¥)")
        Settings.llm = OpenAI(
            model="gpt-4-turbo", 
            temperature=0,  # â¬…ï¸ 0ìœ¼ë¡œ ì„¤ì •!
            api_key=os.getenv("OPENAI_API_KEY")
        )
        Settings.embed_model = OpenAIEmbedding(
            model="text-embedding-3-small", 
            api_key=os.getenv("OPENAI_API_KEY")
        )
    else:
        print("ğŸ”§ ì‹¤ì œ ì±—ë´‡ ëª¨ë“œ: temperature=0.1 (ì•½ê°„ì˜ ë³€ë™ì„±)")
    
    # Agent ì´ˆê¸°í™”
    agent = FDAAgent()
    evaluator = FDAEvaluator()
    
    # â­ ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
    agent.reset_conversation()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ
    test_dataset = get_dataset()
    
    print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(test_dataset)}ê°œ")
    print(f"ì¹´í…Œê³ ë¦¬: {set(t['category'] for t in test_dataset)}")
    print()
    
    # ê° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    for i, test_case in enumerate(test_dataset, 1):
        print(f"\n{'='*80}")
        print(f"[{i}/{len(test_dataset)}] {test_case['id']}")
        print(f"{'='*80}")
        print(f"â“ ì§ˆë¬¸: {test_case['question']}")
        print(f"âœ… ì •ë‹µ: {test_case['ground_truth'][:100]}...")
        print(f"ğŸ”‘ í‚¤ì›Œë“œ: {test_case['expected_keywords']}")
        print()
        
        try:
            # Agent í˜¸ì¶œ
            print("ğŸ¤– Agent ë‹µë³€ ìƒì„± ì¤‘...")
            response = agent.chat(test_case['question'])
            
            # ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°
            content = response.get('content', '')
            print(f"\nğŸ“ ë‹µë³€ (ì²« 200ì):")
            print(f"   {content[:200]}...")
            
            # ê²€ìƒ‰ ë¬¸ì„œ ì¶”ì¶œ (citationsì—ì„œ)
            retrieved_docs = []
            if 'citations' in response:
                print(f"\nğŸ“š ê²€ìƒ‰ëœ ë¬¸ì„œ: {len(response['citations'])}ê°œ")
                for j, citation in enumerate(response['citations'], 1):
                    content = citation.get('content', '')
                    print(f"   [{j}] {citation.get('collection', 'N/A')}: {citation.get('title', 'N/A')[:60]}... (ì ìˆ˜: {citation.get('score', 0):.3f})")
                    
                    retrieved_docs.append({
                        'collection': citation.get('collection', ''),
                        'title': citation.get('title', ''),
                        'score': citation.get('score', 0),
                        'text': content  # â­ ì‹¤ì œ content ì‚¬ìš©
                    })
            
            # í‰ê°€
            print("\nğŸ“Š í‰ê°€ ì‹œì‘...")
            result = evaluator.evaluate_single(
                test_case=test_case,
                agent_response=response,
                retrieved_docs=retrieved_docs
            )
            
            # ê°„ë‹¨í•œ ê²°ê³¼ ì¶œë ¥
            gen = result['generation']
            print(f"\nâœ… í‰ê°€ ì™„ë£Œ:")
            print(f"   - Correctness:  {gen['correctness']:.2f}")
            print(f"   - Faithfulness: {gen['faithfulness']:.2f}")
            print(f"   - Keyword:      {gen['keyword_coverage']:.0%}")
            
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{version_name}_{timestamp}.json"
    
    filepath = evaluator.save_report(filename)
    
    # ìš”ì•½ ì¶œë ¥
    report = evaluator.generate_report()
    
    print("\n" + "="*80)
    print("ğŸ¯ í‰ê°€ ì™„ë£Œ - ìµœì¢… ê²°ê³¼")
    print("="*80)
    
    overall = report['overall_metrics']
    print(f"\nğŸ“Š ì „ì²´ í‰ê·  ì ìˆ˜:")
    print(f"  âœ… Correctness (ì •í™•ì„±):     {overall['correctness']:.3f} / 1.0")
    print(f"  ğŸ“ Faithfulness (ì¶©ì‹¤ë„):    {overall['faithfulness']:.3f} / 1.0")
    print(f"  ğŸ¯ Relevancy (ê´€ë ¨ì„±):       {overall['relevancy']:.3f} / 1.0")
    print(f"  ğŸ” Similarity (ìœ ì‚¬ë„):      {overall['similarity']:.3f} / 1.0")
    print(f"  ğŸ”‘ Keyword Coverage (í‚¤ì›Œë“œ): {overall['keyword_coverage']:.1%}")
    
    # ì „ì²´ í‰ê°€
    avg_score = (overall['correctness'] + overall['faithfulness'] + overall['relevancy']) / 3
    if avg_score >= 0.9:
        grade = "A+ (ìš°ìˆ˜)"
    elif avg_score >= 0.8:
        grade = "A (ì–‘í˜¸)"
    elif avg_score >= 0.7:
        grade = "B+ (ë³´í†µ)"
    else:
        grade = "B (ê°œì„  í•„ìš”)"
    
    print(f"\nğŸ† ì¢…í•© í‰ê°€: {grade}")
    
    print(f"\nğŸ“‚ ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸:")
    for cat, metrics in report['by_category'].items():
        print(f"\n  ğŸ“Œ {cat} ({metrics['count']}ê°œ í…ŒìŠ¤íŠ¸)")
        print(f"     - Correctness:  {metrics['correctness']:.3f}")
        print(f"     - Faithfulness: {metrics['faithfulness']:.3f}")
    
    print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥: {filepath}")
    print(f"ğŸ“ íŒŒì¼ ìœ„ì¹˜: backend/evaluation/results/")
    print("\n" + "="*80)
    
    return report


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='FDA RAG ì‹œìŠ¤í…œ í‰ê°€')
    parser.add_argument(
        '--version',
        type=str,
        default='baseline',
        help='ë²„ì „ ì´ë¦„ (ì˜ˆ: baseline, with_reranking)'
    )
    parser.add_argument(
        '--real-chatbot',
        action='store_true',
        help='ì‹¤ì œ ì±—ë´‡ì²˜ëŸ¼ ë™ì‘ (temperature=0.1, ì•½ê°„ì˜ ë³€ë™ì„± ìˆìŒ)'
    )
    
    args = parser.parse_args()
    
    run_evaluation(args.version, deterministic=not args.real_chatbot)