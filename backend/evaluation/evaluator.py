# evaluation/evaluator.py
"""
LlamaIndex ê¸°ë°˜ RAG í‰ê°€ ì‹œìŠ¤í…œ
"""

import os
import json
from typing import List, Dict, Any
from datetime import datetime

# LlamaIndex í‰ê°€ ëª¨ë“ˆ
from llama_index.core.evaluation import (
    FaithfulnessEvaluator,
    RelevancyEvaluator,
    CorrectnessEvaluator,
    SemanticSimilarityEvaluator
)
from llama_index.core import Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding 


class FDAEvaluator:
    """FDA RAG ì‹œìŠ¤í…œ í‰ê°€ê¸°"""
    
    def __init__(self):
        # í‰ê°€ìš© LLM (ì €ë ´í•œ ëª¨ë¸)
        self.eval_llm = OpenAI(
            model="gpt-4o-mini",
            temperature=0,
            api_key=os.getenv("OPENAI_API_KEY")
        )

        self.eval_embed_model = OpenAIEmbedding(
            model="text-embedding-3-small",
            api_key=os.getenv("OPENAI_API_KEY")
        )

        # LlamaIndex í‰ê°€ì ì´ˆê¸°í™”
        self.faithfulness_evaluator = FaithfulnessEvaluator(llm=self.eval_llm)
        self.relevancy_evaluator = RelevancyEvaluator(llm=self.eval_llm)
        self.correctness_evaluator = CorrectnessEvaluator(llm=self.eval_llm)    
        self.similarity_evaluator = SemanticSimilarityEvaluator(
            embed_model=self.eval_embed_model  # llm â†’ embed_model
        )

        # í‰ê°€ ê²°ê³¼ ì €ì¥
        self.results = []
        
        # ì˜ì–´-í•œêµ­ì–´ í‚¤ì›Œë“œ ë§¤í•‘ (FDA ìš©ì–´)
        self.keyword_translation = {
            # ì£¼ìš” ì•Œë ˆë¥´ê²
            "milk": ["ìš°ìœ ", "ìœ ì œí’ˆ", "milk"],
            "eggs": ["ê³„ë€", "ë‹¬ê±€", "egg", "eggs"],
            "fish": ["ìƒì„ ", "ì–´ë¥˜", "fish"],
            "shellfish": ["ê°‘ê°ë¥˜", "ì¡°ê°œë¥˜", "shellfish", "crustacean"],
            "nuts": ["ê²¬ê³¼ë¥˜", "nuts", "tree nuts"],
            "peanuts": ["ë•…ì½©", "peanut", "peanuts"],
            "wheat": ["ë°€", "ì†Œë§¥", "wheat"],
            "soy": ["ì½©", "ëŒ€ë‘", "soy", "soybeans"],
            "sesame": ["ì°¸ê¹¨", "sesame"],
            "nine": ["9ê°œ", "9ê°€ì§€", "ì•„í™‰", "nine"],
            
            # FDA ê·œì œ ìš©ì–´
            "congress": ["ì˜íšŒ", "êµ­íšŒ", "congress"],
            "cannot": ["í•  ìˆ˜ ì—†", "ë¶ˆê°€", "ê¸ˆì§€", "cannot"],
            "statutory": ["ë²•ì •", "ë²•ë¥ ", "statutory"],
            "section": ["ì„¹ì…˜", "ì¡°í•­", "section"],
            "determined by": ["ê²°ì •", "ì •í•´", "determined"],
            "ingredient list": ["ì„±ë¶„ ëª©ë¡", "ì›ì¬ë£Œëª…", "ingredient"],
            "contains statement": ["í•¨ìœ ", "í¬í•¨", "contains"],
            "declare": ["í‘œì‹œ", "ëª…ì‹œ", "ê¸°ì¬", "declare"],
            "labeling": ["ë¼ë²¨", "í‘œì‹œ", "í‘œê¸°", "labeling"],
            "import alert": ["ìˆ˜ì…ê²½ë³´", "ìˆ˜ì… ê²½ë³´", "import alert"],
            "detention": ["ì–µë¥˜", "detention"],
            "fsvp": ["fsvp", "í•´ì™¸ê³µê¸‰ì—…ì²´ê²€ì¦"],
            "foreign supplier": ["í•´ì™¸ ê³µê¸‰", "ì™¸êµ­ ê³µê¸‰", "foreign supplier"],
            "verification": ["ê²€ì¦", "í™•ì¸", "verification"],
            "importer": ["ìˆ˜ì…ì—…ì", "ìˆ˜ì…ì", "importer"],
            "requirements": ["ìš”êµ¬ì‚¬í•­", "ê·œì •", "ìš”ê±´", "requirements"],
            "registration": ["ë“±ë¡", "registration"],
            "haccp": ["haccp", "í•´ì¹"],
        }
    
    def _check_keyword_in_text(self, keyword: str, text: str) -> bool:
        """í‚¤ì›Œë“œê°€ í…ìŠ¤íŠ¸ì— ìˆëŠ”ì§€ í™•ì¸ (í•œêµ­ì–´ ë²ˆì—­ í¬í•¨)"""
        text_lower = text.lower()
        
        # 1. ì˜ì–´ í‚¤ì›Œë“œ ì§ì ‘ ê²€ìƒ‰
        if keyword.lower() in text_lower:
            return True
        
        # 2. í•œêµ­ì–´ ë²ˆì—­ ê²€ìƒ‰
        if keyword.lower() in self.keyword_translation:
            translations = self.keyword_translation[keyword.lower()]
            for trans in translations:
                if trans.lower() in text_lower:
                    return True
        
        return False
    
    def evaluate_single(
        self, 
        test_case: Dict[str, Any],
        agent_response: Dict[str, Any],
        retrieved_docs: List[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ í‰ê°€"""
        
        print(f"\n{'='*70}")
        print(f"[{test_case['id']}] {test_case['question']}")
        print(f"{'='*70}")
        
        # 1. Retrieval í‰ê°€ (ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°)
        retrieval_metrics = {}
        if retrieved_docs:
            retrieval_metrics = self._evaluate_retrieval(test_case, retrieved_docs)
        
        # 2. Generation í‰ê°€
        generation_metrics = self._evaluate_generation(
            test_case,
            agent_response,
            retrieved_docs
        )
        
        # 3. ì¢…í•© ê²°ê³¼
        result = {
            "test_id": test_case['id'],
            "category": test_case['category'],
            "difficulty": test_case['difficulty'],
            "question": test_case['question'],
            "ground_truth": test_case['ground_truth'],
            
            # Retrieval
            "retrieval": retrieval_metrics,
            
            # Generation
            "generation": generation_metrics,
            
            # ë©”íƒ€
            "timestamp": datetime.now().isoformat(),
            "agent_response": agent_response.get("content", "")[:500]  # ì²˜ìŒ 500ìë§Œ
        }
        
        self.results.append(result)
        
        # ê²°ê³¼ ì¶œë ¥
        self._print_single_result(result)
        
        return result
    
    def _evaluate_retrieval(
        self, 
        test_case: Dict[str, Any],
        retrieved_docs: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """ê²€ìƒ‰ í‰ê°€"""
        
        print(f"\n[Retrieval] í‰ê°€")
        
        # ê¸°ëŒ€í•˜ëŠ” ì»¬ë ‰ì…˜ì´ ê²€ìƒ‰ë˜ì—ˆëŠ”ê°€?
        expected_collections = set(test_case.get('expected_collections', []))
        retrieved_collections = set(doc.get('collection', '') for doc in retrieved_docs if doc.get('collection'))
        
        collection_precision = len(expected_collections & retrieved_collections) / len(retrieved_collections) if retrieved_collections else 0
        collection_recall = len(expected_collections & retrieved_collections) / len(expected_collections) if expected_collections else 0
        
        # ê¸°ëŒ€ í‚¤ì›Œë“œê°€ ê²€ìƒ‰ ê²°ê³¼ì— ìˆëŠ”ê°€? (í•œêµ­ì–´ ë²ˆì—­ í¬í•¨)
        expected_keywords = test_case.get('expected_keywords', [])
        all_text = " ".join([doc.get('text', '') for doc in retrieved_docs])
        
        keyword_hits = sum(1 for kw in expected_keywords if self._check_keyword_in_text(kw, all_text))
        keyword_coverage = keyword_hits / len(expected_keywords) if expected_keywords else 0
        
        # ì ìˆ˜ ë¶„í¬
        scores = [doc.get('score', 0) for doc in retrieved_docs]
        avg_score = sum(scores) / len(scores) if scores else 0
        max_score = max(scores) if scores else 0
        
        metrics = {
            "total_docs": len(retrieved_docs),
            "collection_precision": round(collection_precision, 3),
            "collection_recall": round(collection_recall, 3),
            "keyword_coverage": round(keyword_coverage, 3),
            "avg_score": round(avg_score, 3),
            "max_score": round(max_score, 3),
            "retrieved_collections": list(retrieved_collections)
        }
        
        print(f"  - ê²€ìƒ‰ ë¬¸ì„œ: {metrics['total_docs']}ê°œ")
        print(f"  - Collection Precision: {metrics['collection_precision']:.2%}")
        print(f"  - Keyword Coverage: {metrics['keyword_coverage']:.2%}")
        print(f"  - Avg Score: {metrics['avg_score']:.3f}")
        
        return metrics
    
    def _evaluate_generation(
        self,
        test_case: Dict[str, Any],
        agent_response: Dict[str, Any],
        retrieved_docs: List[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """ë‹µë³€ ìƒì„± í‰ê°€"""
        
        print(f"\n[Generation] í‰ê°€")
        
        query = test_case['question']
        response = agent_response.get("content", "")
        reference = test_case['ground_truth']
        
        # 1. Faithfulness (ë¬¸ì„œ ì¶©ì‹¤ë„) - ê²€ìƒ‰ ë¬¸ì„œê°€ ìˆëŠ” ê²½ìš°ë§Œ
        faithfulness_score = 0.0
        if retrieved_docs:
            try:
                contexts = [doc.get('text', '') for doc in retrieved_docs[:5]]
                
                faithfulness_result = self.faithfulness_evaluator.evaluate(
                    query=query,
                    response=response,
                    contexts=contexts
                )
                faithfulness_score = faithfulness_result.score if faithfulness_result.score else 0.0
                
                print(f"  - Faithfulness: {faithfulness_score:.2f}")
                
            except Exception as e:
                print(f"  [ê²½ê³ ] Faithfulness í‰ê°€ ì‹¤íŒ¨: {e}")
        
        # 2. Relevancy (ë‹µë³€ ê´€ë ¨ì„±)
        relevancy_score = 0.0
        try:
            # contexts íŒŒë¼ë¯¸í„° ì¶”ê°€
            contexts = [doc.get('text', '') for doc in retrieved_docs[:5]] if retrieved_docs else []
            relevancy_result = self.relevancy_evaluator.evaluate(
                query=query,
                response=response,
                contexts=contexts
            )
            relevancy_score = relevancy_result.score if relevancy_result.score else 0.0
            
            print(f"  - Relevancy: {relevancy_score:.2f}")
            
        except Exception as e:
            print(f"  [ê²½ê³ ] Relevancy í‰ê°€ ì‹¤íŒ¨: {e}")
        
        # 3. Correctness (ì •í™•ì„±)
        correctness_score = 0.0
        try:
            correctness_result = self.correctness_evaluator.evaluate(
                query=query,
                response=response,
                reference=reference
            )
            # CorrectnessëŠ” 1-5 ìŠ¤ì¼€ì¼ì´ë¯€ë¡œ 0-1ë¡œ ì •ê·œí™”
            raw_score = correctness_result.score if correctness_result.score else 0.0
            correctness_score = (raw_score - 1) / 4 if raw_score > 0 else 0.0  # 1-5 â†’ 0-1
            
            print(f"  - Correctness: {correctness_score:.2f} (raw: {raw_score:.1f}/5)")
            
        except Exception as e:
            print(f"  [ê²½ê³ ] Correctness í‰ê°€ ì‹¤íŒ¨: {e}")
        
        # 4. Semantic Similarity (ì˜ë¯¸ ìœ ì‚¬ë„)
        similarity_score = 0.0
        try:
            similarity_result = self.similarity_evaluator.evaluate(
                query=query,
                response=response,
                reference=reference
            )
            similarity_score = similarity_result.score if similarity_result.score else 0.0
            
            print(f"  - Semantic Similarity: {similarity_score:.2f}")
            
        except Exception as e:
            print(f"  [ê²½ê³ ] Similarity í‰ê°€ ì‹¤íŒ¨: {e}")
        
        # 5. ì¶”ê°€: í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨ ì²´í¬ (í•œêµ­ì–´ ë²ˆì—­ í¬í•¨)
        expected_keywords = test_case.get('expected_keywords', [])
        keyword_hits = sum(1 for kw in expected_keywords if self._check_keyword_in_text(kw, response))
        keyword_coverage = keyword_hits / len(expected_keywords) if expected_keywords else 0
        
        print(f"  - Keyword Coverage (ë‹µë³€): {keyword_coverage:.2%}")
        if keyword_coverage > 0:
            matched_keywords = [kw for kw in expected_keywords if self._check_keyword_in_text(kw, response)]
            print(f"    ë§¤ì¹­ëœ í‚¤ì›Œë“œ: {matched_keywords}")
        
        return {
            "faithfulness": round(faithfulness_score, 3),
            "relevancy": round(relevancy_score, 3),
            "correctness": round(correctness_score, 3),
            "similarity": round(similarity_score, 3),
            "keyword_coverage": round(keyword_coverage, 3),
            "response_length": len(response)
        }
    
    def _print_single_result(self, result: Dict[str, Any]):
        """ë‹¨ì¼ ê²°ê³¼ ì¶œë ¥"""
        
        print(f"\n[ì™„ë£Œ] í‰ê°€ ì™„ë£Œ")
        print(f"  - Correctness: {result['generation']['correctness']:.2f}")
        print(f"  - Faithfulness: {result['generation']['faithfulness']:.2f}")
        print(f"  - Keyword Coverage: {result['generation']['keyword_coverage']:.2%}")
        
        if result['retrieval']:
            print(f"  - Retrieved: {result['retrieval']['total_docs']}ê°œ")
        
        print()
    
    def generate_report(self) -> Dict[str, Any]:
        """ì „ì²´ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        if not self.results:
            return {"error": "No evaluation results"}
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì§‘ê³„
        by_category = {}
        for result in self.results:
            cat = result['category']
            if cat not in by_category:
                by_category[cat] = []
            by_category[cat].append(result)
        
        # ì „ì²´ í‰ê· 
        def safe_avg(items, key):
            values = [item[key] for item in items if item.get(key) is not None]
            return sum(values) / len(values) if values else 0
        
        generation_metrics = [r['generation'] for r in self.results]
        retrieval_metrics = [r['retrieval'] for r in self.results if r.get('retrieval')]
        
        overall_metrics = {
            "correctness": safe_avg(generation_metrics, 'correctness'),
            "faithfulness": safe_avg(generation_metrics, 'faithfulness'),
            "relevancy": safe_avg(generation_metrics, 'relevancy'),
            "similarity": safe_avg(generation_metrics, 'similarity'),
            "keyword_coverage": safe_avg(generation_metrics, 'keyword_coverage'),
        }
        
        if retrieval_metrics:
            overall_metrics.update({
                "avg_retrieved_docs": safe_avg(retrieval_metrics, 'total_docs'),
                "collection_precision": safe_avg(retrieval_metrics, 'collection_precision'),
                "keyword_coverage_retrieval": safe_avg(retrieval_metrics, 'keyword_coverage'),
            })
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥
        category_performance = {}
        for cat, results in by_category.items():
            gen_metrics = [r['generation'] for r in results]
            category_performance[cat] = {
                "count": len(results),
                "correctness": safe_avg(gen_metrics, 'correctness'),
                "faithfulness": safe_avg(gen_metrics, 'faithfulness'),
            }
        
        return {
            "summary": {
                "total_tests": len(self.results),
                "timestamp": datetime.now().isoformat()
            },
            "overall_metrics": overall_metrics,
            "by_category": category_performance,
            "detailed_results": self.results
        }
    
    def save_report(self, filename: str):
        """ë¦¬í¬íŠ¸ ì €ì¥"""
        
        report = self.generate_report()
        
        os.makedirs("evaluation/results", exist_ok=True)
        filepath = f"evaluation/results/{filename}"
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ë¦¬í¬íŠ¸ ì €ì¥: {filepath}")
        
        return filepath