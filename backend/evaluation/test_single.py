# evaluation/test_single.py
"""
ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import io
sys.path.append('..')

# Windows CMDì—ì„œ UTF-8 ì¶œë ¥ ì§€ì›
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

from evaluation.test_dataset import get_dataset
from evaluation.evaluator import FDAEvaluator
from utils.agent import FDAAgent

# â­ í‰ê°€ìš© ì„¤ì •
import os
from dotenv import load_dotenv
from llama_index.core import Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

load_dotenv()


def test_single_case(test_id: str = "definition_001", deterministic: bool = True):
    """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰
    
    Args:
        test_id: í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ID
        deterministic: Trueì´ë©´ temperature=0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì¼ê´€ëœ ê²°ê³¼ ë³´ì¥
    """
    
    print("="*80)
    print(f"[TEST] {test_id}")
    print("="*80)
    
    # â­ í‰ê°€ìš©ìœ¼ë¡œ temperatureë¥¼ 0ìœ¼ë¡œ ì„¤ì • (ì¼ê´€ëœ ê²°ê³¼ë¥¼ ìœ„í•´)
    if deterministic:
        print("ğŸ”§ í‰ê°€ ëª¨ë“œ: temperature=0 (ì¼ê´€ëœ ê²°ê³¼ ë³´ì¥)")
        Settings.llm = OpenAI(
            model="gpt-4-turbo", 
            temperature=0,  # â¬…ï¸ 0ìœ¼ë¡œ ì„¤ì •!
            api_key=os.getenv("OPENAI_API_KEY")
        )
        Settings.embed_model = OpenAIEmbedding(
            model="text-embedding-3-small", 
            api_key=os.getenv("OPENAI_API_KEY")
        )
    else:
        print("ğŸ”§ ì‹¤ì œ ì±—ë´‡ ëª¨ë“œ: temperature=0.1 (ì•½ê°„ì˜ ë³€ë™ì„±)")
    
    # Agent ë° Evaluator ì´ˆê¸°í™”
    agent = FDAAgent()
    evaluator = FDAEvaluator()
    
    # â­ ë©”ëª¨ë¦¬ ì´ˆê¸°í™” (ì‹¤ì œ ì±—ë´‡ì²˜ëŸ¼ ê¹¨ë—í•œ ìƒíƒœì—ì„œ ì‹œì‘)
    agent.reset_conversation()
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì°¾ê¸°
    test_dataset = get_dataset()
    test_case = next((t for t in test_dataset if t['id'] == test_id), None)
    
    if not test_case:
        print(f"[ERROR] í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_id}")
        return
    
    print(f"\n[Q] ì§ˆë¬¸: {test_case['question']}")
    print(f"[A] ì •ë‹µ: {test_case['ground_truth'][:100]}...")
    print(f"[K] í‚¤ì›Œë“œ: {test_case['expected_keywords']}")
    print()
    
    try:
        # Agent í˜¸ì¶œ
        print("[Agent] ë‹µë³€ ìƒì„± ì¤‘...")
        response = agent.chat(test_case['question'])
        
        print(f"\n[Response] Agent ë‹µë³€:")
        print("-" * 80)
        print(response.get('content', '')[:500])
        print("-" * 80)
        
        # ê²€ìƒ‰ ë¬¸ì„œ ì¶”ì¶œ
        retrieved_docs = []
        if 'citations' in response:
            print(f"\n[Debug] ê²€ìƒ‰ëœ ë¬¸ì„œ ìƒì„¸ ë‚´ìš©:")
            for i, citation in enumerate(response['citations'], 1):
                content = citation.get('content', '')
                print(f"\n  [{i}] {citation.get('title', '')[:100]}")
                print(f"      ğŸ“ ì „ì²´ ê¸¸ì´: {len(content)}ì")
                print(f"      ë‚´ìš© (ì²« 1000ì):")
                print(f"      {content[:1000]}")
                if len(content) > 1000:
                    print(f"      ... (ìƒëµ: {len(content) - 1000}ì)")
                
                retrieved_docs.append({
                    'collection': citation.get('collection', ''),
                    'title': citation.get('title', ''),
                    'score': citation.get('score', 0),
                    'text': content
                })
        
        # í‰ê°€
        print("\n[Evaluation] í‰ê°€ ì‹œì‘...")
        result = evaluator.evaluate_single(
            test_case=test_case,
            agent_response=response,
            retrieved_docs=retrieved_docs
        )
        
        # ìƒì„¸ ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*80)
        print("[RESULT] í‰ê°€ ì™„ë£Œ")
        print("="*80)
        
        gen = result['generation']
        print(f"\n[Generation] í‰ê°€:")
        print(f"  - Correctness:       {gen['correctness']:.3f} / 1.0")
        print(f"  - Faithfulness:      {gen['faithfulness']:.3f} / 1.0")
        print(f"  - Relevancy:         {gen['relevancy']:.3f} / 1.0")
        print(f"  - Similarity:        {gen['similarity']:.3f} / 1.0")
        print(f"  - Keyword Coverage:  {gen['keyword_coverage']:.1%} ({int(gen['keyword_coverage'] * len(test_case['expected_keywords']))}/{len(test_case['expected_keywords'])})")
        
        if result['retrieval']:
            ret = result['retrieval']
            print(f"\n[Retrieval] í‰ê°€:")
            print(f"  - ê²€ìƒ‰ ë¬¸ì„œ:          {ret['total_docs']}ê°œ")
            print(f"  - Keyword Coverage:  {ret['keyword_coverage']:.1%}")
            print(f"  - Avg Score:         {ret['avg_score']:.3f}")
        
        print("\n" + "="*80)
        
        return result
        
    except Exception as e:
        print(f"\n[ERROR] ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰')
    parser.add_argument(
        '--id',
        type=str,
        default='definition_001',
        help='í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ID (ì˜ˆ: definition_001, authority_001)'
    )
    parser.add_argument(
        '--real-chatbot',
        action='store_true',
        help='ì‹¤ì œ ì±—ë´‡ì²˜ëŸ¼ ë™ì‘ (temperature=0.1, ì•½ê°„ì˜ ë³€ë™ì„± ìˆìŒ)'
    )
    
    args = parser.parse_args()
    
    # --real-chatbot í”Œë˜ê·¸ê°€ ìˆìœ¼ë©´ deterministic=False
    test_single_case(args.id, deterministic=not args.real_chatbot)

